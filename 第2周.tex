\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 2}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\section{Multivariate Linear Regression}
\subsection{Multiple Features}
\subparagraph*{}
 根据之前的例子，我们对房价模型增加更多的特征，例如房间数楼层等，构成了一个含有多个变量的模型，模型中的特征值为（$x_1$,$x_2$,...,$x_n$）:
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{221.png}}
\caption{多变量房价模型}
\label{fig:221}
\end{figure}
\subparagraph*{}
注释：
\subparagraph*{}
（1）n：特征的数量
\subparagraph*{}
（2）$x^{(i)}$：第i个训练样本，是特征矩阵中的第i行，是一个向量（vector）；
例如：上图中的
\begin{equation}
x^{(2)}=\left[\begin{matrix}1416\\3\\2\\40\end{matrix}\right]
\end{equation}
\subparagraph*{}
（3）$x_j^{(i)}$：特征矩阵中第i行的第j个特征值，也就是第i个训练样本实例的第j个特征，如上图的$x_3^{(2)}$=2。
\subparagraph*{}
多变量的假设函数：
\subparagraph*{}
\begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
这个公式中有你n+1个参数和n个变量，为了能使公式简化，引入$x_0$=1，则公式转化为：
\begin{equation}
h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
此时模型中的参数是一个n+1维的变量，任何一个训练实例都是n+1维的向量，特征矩阵X的维度是m*(n+1)。
公式可以简化为：
\begin{equation}
h_\theta(x)=\left[\begin{matrix}\theta_0&\theta_1&.&.&.&\theta_n\end{matrix}\right]\left[\begin{matrix}x_0\\x_1\\.\\.\\.\\x_n\end{matrix}\right]=\theta^TX
\end{equation}
\begin{equation}
h_\theta(x)=\theta^TX
\end{equation}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{222.png}}
\caption{多变量假设函数}
\label{fig:222}
\end{figure}
\subsection{Gradient Descent for Multiple Variables}
\subparagraph*{}
与单变量线性回归类似，在多变量线性回归中也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：
\begin{equation}
J(\theta_1,\theta_2,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^m{(h_\theta(x^{(i)})-y^{(i)})^2}
\end{equation}
其中：
\begin{equation}
h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
多变量梯度算法定义：n=1和n>=1的情况其实是一样的
\subparagraph*{}
repeat until convergence:\{{}
\begin{equation}
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_0^{(i)}
\end{equation}
\begin{equation}
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_1^{(i)}
\end{equation}
\begin{equation}
\theta_2:=\theta_2-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_2^{(i)}
\end{equation}
\subparagraph*{}
......
\subparagraph*{}
\}{}
\subparagraph*{}
In others words:
\subparagraph*{}
repeat until convergence:\{{}
\begin{equation}
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_j^{(i)}   
\end{equation}
\begin{equation}
for j:=0,1,2,...,n
\end{equation}
\subparagraph*{}
\}{}
\subparagraph*{}
单变量与多变量的比较：
The following image compares gradient descent with one variable to gradient descent with multiple variables: 
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{223.png}}
\caption{单变量与多变量的梯度下降算法对比}
\label{fig:223}
\end{figure}
\subsection{Gradient Descent in Practice I-Feature Scaling}
\subparagraph*{}
特征缩放可以将梯度下降的速度加快，将梯度下降收敛所需的循环次数减少。
\subparagraph*{}
以房价问题为例，假设我们使用两个特征,房屋的尺寸和房间的数量,尺寸的值为 0-2000 平方英尺,而房间数量的值则是 0-5,以两个参数分别为横纵坐标,绘制代价函数的等高线图，如图所示，代价函数的等高线图很扁，又细又长，梯度下降算法需要非常多次的迭代才能收敛。
\subparagraph*{}
解决的额方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{224.png}}
\caption{多变量假设函数}
\label{fig:224}
\end{figure}
\subparagraph*{}
Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:
\begin{equation}
x_i=\frac{x_i-\mu_i}{s_i}
\end{equation}
\subparagraph*{}
$\mu_i$ :is the average of all the values for feature (i);
\subparagraph*{}
$s_i$ :is the range of values (max - min), or si is the standard deviation(标准差)
\subparagraph*{}
Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.
\subparagraph*{}
For example：if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then：
\begin{equation}
x_i=\frac{price-1000}{900}
\end{equation}
\subsection{Gradient Descent in Practice II-Learning Rate}
\subparagraph*{}
梯度下降算法所需的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数个代价函数的图表来观测算法在何时趋于收敛。
\subparagraph*{}
也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阈值（例如0.001）进行比较，但是通常看如下的图表更好。
\subparagraph*{}
Debugging gradient descent：Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.
\subparagraph*{}
Automatic convergence test：Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it's difficult to choose this threshold value.
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{226.png}}
\caption{迭代次数与代价函数}
\label{fig:226}
\end{figure}
\subparagraph*{}
梯度下降算法的每次迭代受到学习率$\alpha$的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高；如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。
\subparagraph*{}
It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{227.png}}
\caption{$\alpha$的影响}
\label{fig:227}
\end{figure}
\subparagraph*{}
通常可以考虑尝试这些学习率$\alpha$：
\subparagraph*{}
$\alpha$ =0.01，0.03，0.1，0.3，1，3，10
\subparagraph*{}
To summarize:
\subparagraph*{}
If $\alpha$ is too small: slow convergence. 
\subparagraph*{}
If $\alpha$ is too large: ￼may not decrease on every iteration and thus may not converge.
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{225.png}}
\caption{Learning Rate}
\label{fig:225}
\end{figure}
\subsection{Features and Polynomial Regression}
\subparagraph*{}
举例：房价预测问题
\begin{equation}
h_\theta(x)=\theta_0+\theta_1\times{frontage}+\theta_2\times{depth}
\end{equation}
\subparagraph*{}
注释：
\subparagraph*{}
$x_1$:frontage(临街宽度)
\subparagraph*{}
$x_2$:depth(纵向深度)
\subparagraph*{}
$x$:area=frontage*depth(面积)
则：\begin{equation}
h_\theta(x)=\theta_0+\theta_1x
\end{equation}
\subparagraph*{}
线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：\begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2
\end{equation}
或者三次方模型：
\begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3
\end{equation}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{228.png}}
\caption{房价预测：多项式回归}
\label{fig:228}
\end{figure}
\subparagraph*{}
通常我们需要先观察数据然后再决定准备尝试怎样的模型。另外我们可以另：
\begin{equation}
x_2=x_2^2
\end{equation}
\begin{equation}
x_3=x_3^3
\end{equation}
从而将模型转化为线性回归模型。
\subparagraph*{}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{229.png}}
\caption{Choice of features}
\label{fig:229}
\end{figure}
根据函数的图形特征，我们可以使：
\begin{equation}
h_\theta(x)=\theta_0+\theta_1size+\theta_2(size)^2
\end{equation}
\subparagraph*{}
或者：
\begin{equation}
h_\theta(x)=\theta_0+\theta_1size+\theta_2\sqrt{(size)}
\end{equation}
\subparagraph*{}
Polynomial Regression(多项式回归)：
\subparagraph*{}
Our hypothesis function need not be linear (a straight line) if that does not fit the data well.
\subparagraph*{}
We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).
\subparagraph*{}
For example, if our hypothesis function is \begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1
\end{equation}
then we can create additional features based on x1, to get the quadratic function \begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2
\end{equation} or the cubic function \begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_1^3
\end{equation}
\subparagraph*{}
In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2=x_1^2$ and $x_3=x_1^3$.
\subparagraph*{}
To make it a square root function, we could do: $h_\theta(x)=\theta_0+\theta_1x_1+\theta_2\sqrt{x_1}$
\subparagraph*{}
One important thing to keep in mind is, if you choose your features this way then feature scaling(特征缩放) becomes very important.
\subparagraph*{}
注意：如果我们采用多项式回归，在运行梯度下降算法前，特征缩放非常有必要。
\subparagraph*{}
eg. if x1 has range 1 - 1000 then range of $x_1^2$ becomes 1 - 1000000 and that of $x_1^3$ becomes 1 - 1000000000
\subparagraph*{}
多项式回归特征缩放练习题：
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{230.png}}
\caption{特征缩放例题}
\label{fig:230}
\end{figure}
\section{Computer Parameters Analytically}
\subsection{Normal Equation}
正规方程：针对某些问题，正规方程有更好的解决方案。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{231.png}}
\caption{Intuition}
\label{fig:231}
\end{figure}
\subparagraph*{}
正规方程通过求解下面的方程来找出使得代价函数最小的参数的$\theta$:
\begin{equation}
\frac{\partial}{\partial\theta_j}J(\theta_j)=0
\end{equation}
\subparagraph*{}
假使我们的训练集特征为X（包含了$x_0$=1），并且我们的训练集结果为向量y，则利用正规方程解出向量
\begin{equation}
\theta=(X_TX)^{-1}X^Ty
\end{equation}
注释：
上标T代表矩阵转置，上标-1代表矩阵的逆。设矩阵$A=X_TX$,则：$(X_TX)^{-1}=A^{-1}$
\subparagraph*{}
以下数据为例：
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{232.png}}
\caption{房价问题：正规方程}
\label{fig:232}
\end{figure}
\subparagraph*{}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{233.png}}
\caption{数据表格}
\label{fig:233}
\end{figure}
\subparagraph*{}
运用正规方程求解参数：
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{234.png}}
\caption{运用正规方程求解参数}
\label{fig:234}
\end{figure}
\subparagraph*{}
在Octave中，正规方程\begin{equation}
\theta=(X_TX)^{-1}X^Ty
\end{equation}
\subparagraph*{}
写作：
\begin{equation}
pinv(X'*X)*X'*y
\end{equation}
\subparagraph*{}
注意：对于那些不可逆的矩阵(通常是因为特征之间不独立,如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征,也有可能是特征数量大于训练集的数量),正规方程方法是不能用的。
\subparagraph*{}
梯度下降与正规方程的比较：
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{235.png}}
\caption{梯度下降与正规方程的比较}
\label{fig:235}
\end{figure}
\subparagraph*{}
总结:只要特征变量的数目并不大,标准方程是一个很好的计算参数 $\theta$的替代方法。具体地说,只要特征变量数量小于一万,我通常使用标准方程法,而不使用梯度下降法。
\subparagraph*{}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{237.png}}
\caption{Gradient Descent or Normal Equation}
\label{fig:237}
\end{figure}
\subparagraph*{}
With the normal equation, computing the inversion has complexity $\\bigcirc(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.
\subsection{Normal Equation Noninvertibility}
\subparagraph*{}
When implementing the normal equation in octave we want to use the 'pinv' function rather than 'inv.' The 'pinv' function will give you a value of $\theta$ even if $X^TX$ is not invertible. 
\subparagraph*{}
If $X^TX$ is noninvertible, the common causes might be having :
\subparagraph*{}
(1)Redundant features, where two features are very closely related (i.e. they are linearly dependent)
\subparagraph*{}
(2)Too many features (e.g. m ≤ n). In this case, delete some features or use "regularization" (to be explained in a later lesson).
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{238.png}}
\caption{Non-invertible}
\label{fig:238}
\end{figure}
\subparagraph*{}
Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.
\end{CJK}
\end{document}