\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 8}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\part{Unsupervised Learning}
\section{Clustering}
\subparagraph{}
聚类算法：非监督学习算法，要让计算机学习无标签数据，而不是之前的标签数据。
\subsection{Unsupervised Learning:Introduction}
\subparagraph{}
监督学习和无监督学习的比较：
\subparagraph{}
在一个监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，我们的监督学习中，有一系列标签，我们需要据此拟合一个假设函数。
\begin{figure}[H]
\center{\includegraphics[width=.3\textwidth]{802.png}}
\label{fig:802}
\caption{监督学习}
\end{figure}
\subparagraph{}
与此不同的是，在无监督学习中，数据没有附带任何标签，我们拿到的数据就是这样的：
\begin{figure}[H]
\center{\includegraphics[width=.3\textwidth]{801.png}}
\label{fig:801}
\caption{无监督学习}
\end{figure}
\subparagraph{}
在这里我们有一系列的点，却没有任何标签，因此我们的训练集可以写成只有$x_1,x_2,x_3,...,x_m$,没有任何标签y。因此图上画的这些点没有标签信息，也就是说，在无监督学习中，我们需要将一系列无标签的训练数据，输入到下一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，被称为聚类算法。
\subparagraph{}
聚类算法使我们介绍的第一个无监督学习算法，以后还将提到其他的无监督学习算法，它可以为我们找到其他类型的结构或者其他的一些模式，而不是簇。
\subparagraph{}
聚类算法是用来做什么的？
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{803.png}}
\label{fig:803}
\caption{聚类算法的应用}
\end{figure}
\begin{itemize}
\item 市场分割:也许你在数据库中分别销售产品或者分别提供更合适的服务，而你希望他们分为不同的课客户群；
\item 社交网络分析：例如 Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群，因此，这可能需要另一个聚类算法，你希望用它发现社交网络中关系密切的朋友。
\item 管理数据中心：使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心，因为数据中心中，了解了哪些计算机数据中心更倾向于一起协作工作，那么就可以重新分配资源，重新布局网络，由此优化数据中心，优化数据通信。
\item 天文学：利用聚类算法视图了解星系的形成和其中的天文学的具体细节。
\end{itemize}
\begin{figure}[H]
\center{\includegraphics[width=.7\textwidth]{805.png}}
\label{fig:805}
\end{figure}
\subsection{K-Means Algorithm}
\subparagraph{}
K-均值算法是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。
\subparagraph{}
执行K-均值算法，首先先随机选择两个点，这两个点叫做聚类中心，就是图上的两个叉，为什么是两个点呢，因为我们希望聚出两个类。
\subparagraph{}
K-均值是一个迭代方法，需要做两件事情：第一是簇分配，第二是移动聚类中心。
\subparagraph{}
簇分配：需要遍历所有的样本，就是图上绿色的点，然后依据每一个点是更接近红色的这个中心，还是更接近绿色的这个中心，来将每个数据点分配到两个不同的聚类中心中。
\begin{figure}[H]
\center{\includegraphics[width=.3\textwidth]{806.png}}
\label{fig:806}
\caption{簇分配}
\end{figure}
\subparagraph{}
具体来讲，就是对对数据集中的所有点，根绝他们更接近红色这个中心，还是蓝色这个中心，进行染色，染色的结果如图：
\begin{figure}[H]
\center{\includegraphics[width=.3\textwidth]{807.png}}
\label{fig:807}
\caption{簇分配染色}
\end{figure}
\subparagraph{}
移动聚类中心:将两个聚类中心，也就是说红色的叉和蓝色的叉移动到 和它一样颜色的那堆点的均值处,找出所有红色的点,计算出它们的均值,就是所有红色的点平均下来的位置,然后把红色点的聚类中心移动到这里,蓝色也是这样,找出所有蓝色的点计算它们的均值把蓝色的叉放到那里,我们将按照图上所示这么移动:
\begin{figure}[H]
\center{\includegraphics[width=.3\textwidth]{808.png}}
\label{fig:808}
\caption{移动聚类中心}
\end{figure}
\subparagraph{}
重复上述步骤，继续簇分配和移动聚类中心，一直迭代下去，直到最后聚类中心不会变，并且哪些点的颜色也不会变，在这时我们就可以说 K-均值方法已经收敛了，在这些数据中找到两个簇。
\subparagraph{}
K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法是：
\subparagraph{}
首先选择K个随机的点，称为聚类中心（cluster centroids）；
\subparagraph{}
对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。
\subparagraph{}
计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
\subparagraph{}
重复步骤直至中心点不再变化。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{809.png}}
\label{fig:809}
\caption{K-均值算法}
\end{figure}
\subparagraph{}
$c^{(i)}$ 是距离样本$x^{(i)}$ 的距离的平方，使距离最小或者距离的平方最小都能让我们得到一个相同的$c^{(i)}$，注意$c^{(i)}$ 是一个在1到K之间的数。（通常还是写成距离的平方）
\begin{figure}[H]
\center{\includegraphics[width=.7\textwidth]{810.png}}
\label{fig:810}
\caption{K-均值算法}
\end{figure}
\subparagraph{}
算法分为两个步骤,第一个 for 循环是赋值步骤,即:对于每一个样例 i,计算其应该属于的类。第二个 for 循环是聚类中心的移动,即:对于每一个类 k,重新计算该类的聚类中心。
\subparagraph{}
K-均值算法也可以很便利地用于将数据分为许多不同组,即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的,利用 K-均值算法将数据分为三类,用于帮助确定将要生产的T-恤衫的三种尺寸 S,M,L。
\begin{figure}[H]
\center{\includegraphics[width=.7\textwidth]{812.png}}
\label{fig:812}
\caption{K-means for non-separated clusters}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{811.png}}
\label{fig:811}
\end{figure}
\subsection{Optimization Objective}
\subparagraph{}
K-均值最小化问题,是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和,因此 K-均值的代价函数(又称畸变函数 Distortion function)为:
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{817.png}}
\label{fig:817}
\caption{优化目标函数}
\end{figure}
\subparagraph{}
其中$\mu_{c^{(i)}}$代表与$x^{(i)}$ 最近的聚类中心点;
\subparagraph{}
每个样本到每个样本所属簇的距离的平方;
\subparagraph{}
K:簇的总数；
\subparagraph{}
k：聚类中心的符号，$k\in{{1,2,3,...,K}}$
\begin{figure}[H]
\center{\includegraphics[width=.7\textwidth]{813.png}}
\label{fig:813}
\caption{K-means algorithm}
\end{figure}
\subparagraph{}
回顾刚才给出的 K-均值迭代算法,我们知道,第一个循环是用于减小 $c^{(i)}$ 引起的代价,而第二个循环则是用于减小$\mu_i$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数,不然便是出现了错误。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{814.png}}
\label{fig:814}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{815.png}}
\label{fig:815}
\end{figure}
\subsection{Random Initialization}
\subparagraph{}
在运行 K-均值算法的之前,我们首先要随机初始化所有的聚类中心点,下面介绍怎样做:
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{816.png}}
\label{fig:816}
\caption{聚类算法}
\end{figure}
\subparagraph{}
1. 我们应该选择 K<m,即聚类中心点的个数要小于所有训练集实例的数量
\subparagraph{}
2. 随机选择 K 个训练实例,然后令 K 个聚类中心分别与这 K 个训练实例相等
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{817.png}}
\label{fig:817}
\caption{随机初始化}
\end{figure}
\subparagraph{}
K-均值的一个问题在于,它有可能会停留在一个局部最小值处,而这取决于初始化的情况。
\subparagraph{}
为了解决这个问题,我们通常需要多次运行 K-均值算法,每一次都重新进行随机初始化,最后再比较多次运行 K-均值的结果,选择代价函数最小的结果。这种方法在 K 较小的时候(2--10)还是可行的,但是如果 K 较大,这么做也可能不会有明显地改善。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{818.png}}
\label{fig:818}
\caption{随机初始化算法}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{820.png}}
\label{fig:820}
\caption{聚类算法}
\end{figure}
\subsection{Choosing the Number of Clusters}
\subparagraph{}
如何选择聚类数目或者如何选择大写K的值。
\subparagraph{}
没有所谓最好的选择聚类数的方法,通常是需要根据不同的问题,人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么,然后选择能最好服务于该目的标聚类数。
\subparagraph{}
当人们在讨论,选择聚类数目的方法时,有一个可能会谈及的方法叫作“肘部法则”（Elbow Method）。关于“肘部法则”,我们所需要做的是改变 K 值,也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着,所有的数据都会分到一个聚类里,然后计算成本函数或者计算畸变函数 J。K 代表聚类数字。
\subparagraph{}
我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的,让我们来看这样一个图,看起来就好像有一个很清楚的肘在那儿。好像人的手臂,如果你伸出你的胳膊,那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{822.png}}
\label{fig:822}
\caption{肘部法则}
\end{figure}
\subparagraph{}
你会发现这种模式,它的畸变值会迅速下降,从 1 到 2,从 2 到 3 之后,你会在 3 的时候达到一个肘点。在此之后,畸变值就下降的非常慢,看起来就像使用 3 个聚类来进行聚类是正确的,这是因为那个点是曲线的肘点,畸变值下降得很快,K 等于 3 之后就下降得很慢,那么我们就选 K 等于 3。当你应用“肘部法则”的时候,如果你得到了一个像上面这样的图,那么这将是一种用来选择聚类个数的合理方法。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{821.png}}
\label{fig:821}
\end{figure}
\subparagraph{}
后续的目的：选择聚类算法要符合你后续的目的。
\subparagraph{}
例如,我们的 T-恤制造例子中,我们要将用户按照身材聚类,我们可以分成 3 个尺寸S,M,L 也可以分成 5 个尺寸 XS,S,M,L,XL,这样的选择是建立在回答“聚类后我们制造的 T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{823.png}}
\label{fig:823}
\caption{聚类算法}
\end{figure}
\subparagraph{}
再比如做图像压缩这个例子，就要根据图片压缩的具体标准选择你需要的聚类数K。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{825.png}}
\label{fig:825}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{826.png}}
\label{fig:826}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{827.png}}
\label{fig:827}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{828.png}}
\label{fig:828}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{829.png}}
\label{fig:829}
\end{figure}
\part{Dimensionality Reduction}
\section{Motivation}
\subsection{Motivation I:Compression}
\subparagraph{}
这个视频,我想开始谈论第二种类型的无监督学习问题,称为降维。有几个不同的的原因使你可能想要做降维。一是数据压缩,数据压缩不仅允许我们压缩数据,因而使用较少的计算机内存或磁盘空间,但它也让我们加快我们的学习算法。
\subparagraph{}
但首先,让我们谈论降维是什么。作为一种生动的例子,我们收集的数据集,有许多,许多特征,我绘制在这里。
\subparagraph{}
假设我们未知两个的特征 $x_1$ :长度:用厘米表示;$x_2$,是用英寸表示同一物体的长度。
\subparagraph{}
所以,这给了我们高度冗余表示,也许不是两个分开的特征 $x_1$ 和$x_2$ ,这两个基本的长度度量,也许我们想要做的是减少数据到一维,只有一个数测量这个长度。这个例子似乎有点做作,这里厘米英寸的例子实际上不是那么不切实际的,两者并没有什么不同。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{830.png}}
\label{fig:830}
\caption{降维}
\end{figure}
\subparagraph{}
将数据从二维降至一维: 假使我们要采用两种不同的仪器来测量一些东西的尺寸,其中一个仪器测量结果的单位是英寸,另一个仪器测量的结果是厘米,我们希望将测量的结果作为我们机器学习的特征。现在的问题的是,两种仪器对同一个东西测量的结果不完全相等(由于误差、精度等),而将两者都作为 特征有些重复,因而,我们希望将这个二维的数据降至一维。
\subparagraph{}
从这件事情我看到的东西发生在工业上的事。如果你有几百个或成千上万的特征,它是它这往往容易失去你需要的特征。有时可能有几个不同的工程团队,也许一个工程队给你二百个特征,第二工程队给你另外三百个的特征,第三工程队给你五百个特征,一千多个特征都在一起,它实际上会变得非常困难,去跟踪你知道的那些特征,你从那些工程队得到的。其实不想有高度冗余的特征一样。
\subparagraph{}
多年我一直在研究直升飞机自动驾驶。诸如此类。如果你想测量——如果你想做,你知道,做一个调查或做这些不同飞行员的测试——你可能有一个特征:$x_1$ ,这也许是他们的技能(直升机飞行员),也许“$x_2$ ”可能是飞行员的爱好。这是表示他们是否喜欢飞行,也许这两个特征将高度相关。你真正关心的可能是这条红线的方向,不同的特征,决定飞行员的能力。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{832.png}}
\label{fig:832}
\caption{数据压缩}
\end{figure}
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{833.png}}
\label{fig:833}
\caption{三维降二维}
\end{figure}
将数据从三维降至二维: 这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的,我们将三维向量投射到一个二维的平面上,强迫使得所有的数据都在同一个平面上,降至二维的特征向量。
\subparagraph{}
这样的处理过程可以被用于把任何维度的数据降到任何想要的维度,例如将 1000 维的特征降至 100 维。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{834.png}}
\label{fig:834}
\end{figure}
\subparagraph{}
正如我们所看到的,最后,这将使我们能够使我们的一些学习算法运行也较晚,但我们会在以后的视频提到它。
\subsection{Motivation II:Visualization}
\subparagraph{}
可视化数据：在许多及其学习问题中,如果我们能将数据可视化,我们便能寻找到一个更好的解决方案,降维可以帮助我们。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{835.png}}
\label{fig:835}
\end{figure}
假使我们有有关于许多不同国家的数据,每一个特征向量都有 50 个特征(如,GDP,人均 GDP,平均寿命等)。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维,我们便可以将其可视化了。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{836.png}}
\label{fig:836}
\caption{可视化数据}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{837.png}}
\label{fig:837}
\caption{可视化数据}
\end{figure}
\subparagraph{}
这样做的问题在于,降维的算法只负责减少维数,新产生的特征的意义就必须由我们自己去发现了。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{838.png}}
\label{fig:838}
\end{figure}
\section{Principal Component Analysis}
\subparagraph{}
主成分分析法（PCA）
\subsection{Principal Component Analysis Problem Formulation}
公式表达
\subparagraph{}
主成分分析(PCA)是最常见的降维算法。
\subparagraph{}
在 PCA 中,我们要做的是找到一个方向向量(Vector direction),当我们把所有的数据都投射到该向量上时,我们希望投射平均均方误差能尽可能地小。（蓝色小线段的距离最小）也就是投影误差最小，方向向量是一个经过原点的向量,而投射误差是从特征向量向该方向向量作垂线的长度。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{840.png}}
\label{fig:840}
\caption{投影误差}
\end{figure}
\subparagraph{}
下面给出主成分分析问题的描述:
\subparagraph{}
问题是要将 n 维数据降至 k 维,目标是找到向量 u (1) ,u (2) ,...,u (k) 使得总的投影误差最小。
（寻找使数据点到投影后的点之间的距离最小的向量）
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{841.png}}
\label{fig:841}
\caption{PCA}
\end{figure}
\subparagraph{}
主成分分析与线性回顾的比较:
\subparagraph{}
主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差(Projected Error)（最小的直角距离）,而线性回归尝试的是最小化预测误差(某个点与通过假设得到的预测值之间的距离)。线性回归的目的是预测结果,而主成分分析不作任何预测。线性回归有预测值y，而PCA的每个样本都被同等的对待。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{842.png}}
\label{fig:842}
\caption{线性回归与PCA}
\end{figure}
\subparagraph{}
上图中,左边的是线性回归的误差(垂直于横轴投影),右边则是主要成分分析的误差(垂直于红线投影)。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{844.png}}
\label{fig:844}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{845.png}}
\label{fig:845}
\end{figure}
\subparagraph{}
PCA 将 n 个特征降维到 k 个,可以用来进行数据压缩,如果 100 维的向量最后可以用 10维来表示,那么压缩率为 90\%{}。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA要保证降维后,还要保证数据的特性损失最小。
\subparagraph{}
PCA 技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序,根据需要取前面最重要的部分,将后面的维数省去,可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。
\subparagraph{}
PCA 技术的一个很大的优点是,它是完全无参数限制的。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预,最后的结果只与数据相关,与用户是独立的。
\subparagraph{}
但是,这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识,掌握了数据的一些特征,却无法通过参数化等方法对处理过程进行干预,可能会得不到预期的效果,效率也不高。
\subsection{Principal Component Analysis Algorithm}
\subparagraph{}
主成分分析PCA的算法：
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{846.png}}
\label{fig:846}
\caption{数据预处理}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{847.png}}
\label{fig:847}
\caption{PCA算法降维}
\end{figure}
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{849.png}}
\label{fig:849}
\caption{PCA算法}
\end{figure}
PCA降维： n 维到 k 维:
\subparagraph{}
第一步是均值归一化。我们需要计算出所有特征的均值,然后令$x_j=x_j-\mu_j$。如果特征是在不同的数量级上,我们还需要将其除以标准差$\sigma^2$。
\subparagraph{}
第二步是计算协方差矩阵(covariance matrix)Σ:
\begin{equation}
\Sigma=\frac{1}{m}\sum_{i=1}^n{(x^{(i)}){(x^{(i)})}^T}
\end{equation}
\subparagraph{}
第三步是计算协方差矩阵 Σ 的特征向量(eigenvectors):
\subparagraph{}
在 Octave 里我们可以利用奇异值分解(singular value decomposition)来求解, [U, S, V]=
svd(sigma)。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{850.png}}
\label{fig:850}
\caption{PCA算法 }
\end{figure}
\subparagraph{}
x可以是训练集中的样本，可以是交叉验证集中的样本，也可以是测试集中的样本。
\subparagraph{}
对于一个 n×n 维度的矩阵,上式中的 U 是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从 n 维降至 k 维,我们只需要从 U 中选取前 K 个向量,获得一个 n×k 维度的矩阵,我们用 $U_reduce$ 表示,然后通过如下计算获得要求的新特征向量$z^{(i)}$:
\begin{equation}
z^{(i)}=U_{reduce}^T\times{x^{(i)}}
\end{equation}
\subparagraph{}
其中 x 是 n×1 维的,因此结果为 k×1 维度。注:我们不对方差特征进行处理。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{851.png}}
\label{fig:851}
\caption{PCA算法总结}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{852.png}}
\label{fig:852}
\end{figure}
\section{Applying PCA}
\subsection{Reconstruction from Compressed Representation}
\subparagraph{}
原始数据的重构：
\subparagraph{}
在以前的视频中,我谈论 PCA 作为压缩算法。在那里你可能需要把 1000 维的数据压缩100 维特征,或具有三维数据压缩到一二维表示。所以,如果这是一个压缩算法,应该能回到这个压缩表示,回到你原有的高维数据的一种近似。
\subparagraph{}
所以,给定的,这可能 100 维,怎么回到你原来的表示 x,这可能是 1000 维的数组?
\subparagraph{}
PCA 算法,我们可能有一个这样的样本。如图中样本 $x^{(1)},x^{(2)}$ 。我们做的是,我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数,比如 $Z^{(1)}$ ,指定这些点的位置后他们被投射到这一个三维曲面。给定一个点 $Z^{(1)}$ ,我们怎么能回去这个原始的二维空间呢?x 为 2 维,z 为 1 维,$z=U_{reduce}^Tx$相反的方程为:$x_{approx}=U_{reduce}\cdot{z}$,$x_{approx}\approx{x}$
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{853.png}}
\label{fig:853}
\caption{原始数据的重构}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{854.png}}
\label{fig:854}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{855.png}}
\label{fig:855}
\end{figure}
\subparagraph{}
这已经与原始数据相当相似了。所以,这就是你从低维表示 Z 回到未压缩的表示。我们得到的数据的一个之间你的原始数据 X,我们也把这个过程称为重建原始数据。
\subparagraph{}
当我们认为试图重建从压缩表示 x 的初始值。所以,给定未标记的数据集,您现在知道如何应用 PCA,你的带高维特征 X 和映射到这的低维表示 Z。这个视频,希望你现在也知道如何采取这些低维表示 Z,映射到备份到一个近似你原有的高维数据。
\subparagraph{}
现在你知道如何实施应用 PCA,我们将要做的事是谈论一些技术在实际使用 PCA 很好,特别是,在接下来的视频中,我想谈一谈关于如何选择 k（PCA算法的一个重要参数，代表着主成分的数量，簇的总数）。
\subsection{Choosing the Number of Principal Components}
\subparagraph{}
主要成分分析是减少投影的平均均方误差:
\begin{equation}
\frac{1}{m}\sum_{i=1}^m{\parallel{x^{(i)}-x_{approx}^{(i)}}\parallel}^2
\end{equation}
\subparagraph{}
训练集的方差为:
\begin{equation}
\frac{1}{m}\sum_{i=1}^m{\parallel{x^{(i)}}\parallel}^2
\end{equation}
\subparagraph{}
我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{860.png}}
\caption{选择参数k}
\label{fig:860}
\end{figure}
\subparagraph{}
要告诉别人，你保留了多少主成分，我选择了参数k，使得99\%{}的差异性得以保留。
\subparagraph{}
如果我们希望这个比例小于 1\%{},就意味着原本数据的偏差有 99\%{}都保留下来了,如果我们选择保留 95\%{}的偏差,便能非常显著地降低模型中特征的维度了,95\%{}～99\%{}更为常用。
\subparagraph{}
我们可以先令 K=1,然后进行主要成分分析,获得$U_{reduce}$ 和 z,然后计算比例是否小于1\%{}。如果不是的话再令 K=2,如此类推,直到找到可以使得比例小于 1\%{}的最小 K 值(原因是各个特征之间通常情况存在某种相关性)。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{861.png}}
\caption{选择参数k算法}
\label{fig:861}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{863.png}}
\caption{选择参数k算法总结}
\label{fig:863}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{862.png}}
\label{fig:862}
\end{figure}
\subsection{Advice for Applying PCA}
\subparagraph{}
假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习,即总共有 10000 个特征。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{870.png}}
\caption{监督学习算法加速}
\label{fig:870}
\end{figure}
\subparagraph{}
1. 第一步是运用主要成分分析将数据压缩至 1000 个特征
\subparagraph{}
2. 然后对训练集运行学习算法
\subparagraph{}
3. 在预测时,采用之前学习而来的 $U_{reduce}$ 将输入的特征 x 转换成特征向量 z,然后再进行预测
\subparagraph{}
注意: PCA 定义了从 x到z的对应关系 这种从 x 到 z的对应关系只可以通过在训练集上运行 PCA 定义出来,具体来讲 这种PCA所学习出的对应关系,所做的就是计算出一系列的参数, 这就是特征缩放和均值归一化 ,同时也计算出这样一个降维的矩阵$U_{reduce}$, 但是降维矩阵$U_{reduce}$ 中的数据 就像一个, PCA所学习的参数一样 ,我们需要使我们的参数唯一地适应这些训练集 ,而不是适应我们的交叉验证或者测试集，因此$U_{reduce}$矩阵中的数据就应该只通过对训练集运行PCA来获得。
\subparagraph{}
注:如果我们有交叉验证集和测试集,也采用对训练集学习而来的 $U_{reduce}$。
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{873.png}}
\caption{PCA的应用}
\label{fig:873}
\end{figure}
\subparagraph{}
错误的主成分分析情况:一个常见错误使用主要成分分析的情况是,将其用于减少过拟合(减少了特征的数量)。这样做非常不好,不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征,它并不考虑任何与结果变量有关的信息,因此可能会丢失非常重要的特征。然而当我们进行正则化处理时,会考虑到结果变量,不会丢掉重要的数据。
\subparagraph{}
另一个常见的错误是,默认地将主成分分析作为学习过程中的一部分,这虽然很多时候有效果,最好还是从所有原始特征开始,只在有必要的时候(算法运行太慢或者占用太多内存)才考虑采用主成分分析。
\subparagraph{}
建议: 一开始不要将PCA方法就直接放到算法里 ,先使用原始数据$x^{(i)}$看看效果 ,只有一个原因 ,让我们相信算法出现了问题 ,那就是你的学习算法 ,收敛地非常缓慢 ,占用内存, 或者硬盘空间非常大, 所以你想来压缩数据, 只有当你的$x^{(i)}$效果不好, 只有当你有证据或者充足的理由来确定$x^{(i)}$效果不好的时候 ,那么就考虑用PCA来进行压缩数据 。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{874.png}}
\caption{错误使用}
\label{fig:874}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{880.png}}
\label{fig:880}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{881.png}}
\label{fig:881}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{888.png}}
\label{fig:888}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{883.png}}
\label{fig:883}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{884.png}}
\label{fig:884}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{885.png}}
\label{fig:885}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{886.png}}
\label{fig:886}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{887.png}}
\label{fig:887}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{889.png}}
\label{fig:889}
\end{figure}
\end{CJK}
\end{document}