\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 2}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\section{Multivariate Linear Regression}
\subsection{Multiple Features}
\subparagraph*{}
 根据之前的例子，我们对房价模型增加更多的特征，例如房间数楼层等，构成了一个含有多个变量的模型，模型中的特征值为（$x_1$,$x_2$,...,$x_n$）:
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{221.png}}
\caption{多变量房价模型}
\label{fig:221}
\end{figure}
\subparagraph*{}
注释：
\subparagraph*{}
（1）n：特征的数量
\subparagraph*{}
（2）$x^{(i)}$：第i个训练样本，是特征矩阵中的第i行，是一个向量（vector）；
例如：上图中的
\begin{equation}
x^{(2)}=\left[\begin{matrix}1416\\3\\2\\40\end{matrix}\right]
\end{equation}
\subparagraph*{}
（3）$x_j^{(i)}$：特征矩阵中第i行的第j个特征值，也就是第i个训练样本实例的第j个特征，如上图的$x_3^{(2)}$=2。
\subparagraph*{}
多变量的假设函数：
\subparagraph*{}
\begin{equation}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
这个公式中有你n+1个参数和n个变量，为了能使公式简化，引入$x_0$=1，则公式转化为：
\begin{equation}
h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
此时模型中的参数是一个n+1维的变量，任何一个训练实例都是n+1维的向量，特征矩阵X的维度是m*(n+1)。
公式可以简化为：
\begin{equation}
h_\theta(x)=\left[\begin{matrix}\theta_0&\theta_1&.&.&.&\theta_n\end{matrix}\right]\left[\begin{matrix}x_0\\x_1\\.\\.\\.\\x_n\end{matrix}\right]=\theta^TX
\end{equation}
\begin{equation}
h_\theta(x)=\theta^TX
\end{equation}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{222.png}}
\caption{多变量假设函数}
\label{fig:222}
\end{figure}
\subsection{Gradient Descent for Multiple Variables}
\subparagraph*{}
与单变量线性回归类似，在多变量线性回归中也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：
\begin{equation}
J(\theta_1,\theta_2,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^m{(h_\theta(x^{(i)})-y^{(i)})^2}
\end{equation}
其中：
\begin{equation}
h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{equation}
\subparagraph*{}
多变量梯度算法定义：n=1和n>=1的情况其实是一样的
\subparagraph*{}
repeat until convergence:\{{}
\begin{equation}
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_0^{(i)}
\end{equation}
\begin{equation}
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_1^{(i)}
\end{equation}
\begin{equation}
\theta_2:=\theta_2-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_2^{(i)}
\end{equation}
\subparagraph*{}
......
\subparagraph*{}
\}{}
\subparagraph*{}
In others words:
\subparagraph*{}
repeat until convergence:\{{}
\begin{equation}
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m({h_\theta(x^{(i)})-y^{(i)}})x_j^{(i)}   
\end{equation}
\begin{equation}
for j:=0,1,2,...,n
\end{equation}
\subparagraph*{}
\}{}
\subparagraph*{}
单变量与多变量的比较：
The following image compares gradient descent with one variable to gradient descent with multiple variables: 
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{223.png}}
\caption{单变量与多变量的梯度下降算法对比}
\label{fig:223}
\end{figure}
\subsection{Gradient Descent in Practice I-Feature Scaling}
\subparagraph*{}
特征缩放可以将梯度下降的速度加快，将梯度下降收敛所需的循环次数减少。
\subparagraph*{}
以房价问题为例，假设我们使用两个特征,房屋的尺寸和房间的数量,尺寸的值为 0-2000 平方英尺,而房间数量的值则是 0-5,以两个参数分别为横纵坐标,绘制代价函数的等高线图，如图所示，代价函数的等高线图很扁，又细又长，梯度下降算法需要非常多次的迭代才能收敛。
\subparagraph*{}
解决的额方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{224.png}}
\caption{多变量假设函数}
\label{fig:224}
\end{figure}
\subparagraph*{}
Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:
\begin{equation}
x_i=\frac{x_i-\mu_i}{s_i}
\end{equation}
\subparagraph*{}
$\mu_i$ :is the average of all the values for feature (i);
\subparagraph*{}
$s_i$ :is the range of values (max - min), or si is the standard deviation(标准差)
\subparagraph*{}
Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.
\subparagraph*{}
For example：if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then：
\begin{equation}
x_i=\frac{price-1000}{900}
\end{equation}
\subsection{Gradient Descent in Practice II-Learning Rate}

\subsection{Features and Polynomial Regression}

\section{Computer Parameters Analytically}

\subsection{Normal Equation}

\subsection{Normal Equation Noninvertibility}

\section{Submitting Programming Assignments}

\subsection{Working on and Submitting  Programming}

\subsection{Programming tips from Mentors}

\end{CJK}
\end{document}