\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 6}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\section{Evalutaing a Learning Algorithm}
\subsection{Deciding What to Try Next}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{601.png}}
\label{fig:601}
\end{figure}
\subparagraph{}
机器学习诊断法：
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{602.png}}
\caption{机器学习诊断法}
\label{fig:602}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{603.png}}
\label{fig:603}
\end{figure}
\subsection{Evaluating a Hypothesis}
\subparagraph{}
怎样用你学过的算法来评估假设函数。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{604.png}}
\label{fig:604}
\end{figure}
\subparagraph{}
当确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人
认为得到一个非常小的训练误差一定是一件好事，但已经知道，仅仅是因为这个假设具
有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且也学习了过拟合假
设函数的例子，所以这推广到新的训练集上是不适用的。
\subparagraph{}
那么，该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假
设函数 h(x) 进行画图，然后观察图形趋势,但对于特征变量不止一个的这种一般情况，还
有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可
能实现。
\subparagraph{}
因此，需要另一种方法来评估我们的假设函数过拟合检验。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{604.png}}
\caption{训练集和测试集}
\label{fig:604}
\end{figure}
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{606.png}}
\label{fig:606}
\end{figure}
\subparagraph{}
按如下步骤训练和测试学习算法：
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{608.png}}
\caption{训练集和测试集}
\label{fig:608}
\end{figure}
\subsubsection{Evaluating a Hypothesis}
\subparagraph{}
Once we have done some trouble shooting for errors in our predictions by: 
\begin{itemize}
\item Getting more training examples
\item Trying smaller sets of features
\item Trying additional features
\item Trying polynomial features
\item Increasing or decreasing λ
\end{itemize}
\subparagraph{}
We can move on to evaluate our new hypothesis. 
\subparagraph{}
A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70 \%{} of your data and the test set is the remaining 30 \%{}. 
\paragraph{}
The new procedure using these two sets is then:
\subparagraph{}
1.Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set
\subparagraph{}
2.Compute the test set error $J_{test}(\Theta)$
\subsubsection{The test set error}
\subparagraph{}
1.For linear regression，利用测试集数据计算代价函数：
\begin{equation}
J_{test}(\Theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\Theta(x_{test}^{(i)})-y_{test}^{(i)})^2
\end{equation}
\subparagraph{}
2.For classification ~ Misclassification error (aka 0/1 misclassification error):（误分率）
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{609.png}}
\label{fig:609}
\end{figure}
\subparagraph{}
然后对计算结果求平均。
\subparagraph{}
This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:
\begin{equation}
{Test}\quad{Error}=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_\Theta(x_{test}^{(i)}),y_{test}^{(i)})
\end{equation}
\subparagraph{}
This gives us the proportion of the test data that was misclassified.
\subsection{Model Section and Train/Validation/Test Sets}
\subparagraph{}
（模型选择）怎样选择正确的特征来构造学习算法或者正确选择学习算法中的正则化参数$\lambda$：
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{610.png}}
\label{fig:610}
\caption{过拟合示例}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{611.png}}
\label{fig:611}
\caption{模型选择}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{612.png}}
\label{fig:612}
\caption{训练集，交叉验证集和测试集}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{613.png}}
\label{fig:613}
\caption{训练，交叉验证和测试误差}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{615.png}}
\label{fig:615}
\caption{模型选择}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{616.png}}
\label{fig:616}
\end{figure}
\subparagraph{}
Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. 
\subparagraph{}
Given many models with different polynomial degrees, we can use a systematic approach to identify the 'best' function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.
\subparagraph{}
One way to break down our dataset into the three sets is:
\begin{itemize}
\item Training set: 60%
\item Cross validation set: 20%
\item Test set: 20%
\end{itemize}
\subparagraph{}
We can now calculate three separate error values for the three different sets using the following method:
\begin{itemize}
\item     Optimize the parameters in Θ using the training set for each polynomial degree.
\item Find the polynomial degree d with the least error using the cross validation set.
\item Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);
\subparagraph{}
This way, the degree of the polynomial d has not been trained using the test set.
\end{itemize}
\section{Bias vs. Variance}
\subsection{Diagnosing Bias vs. Variance}
\subparagraph{}
判断一个算法是偏差还是方差问题：
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{617.png}}
\label{fig:617}
\caption{Bias/variance}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{619.png}}
\label{fig:619}
\caption{Bias/variance error}
\end{figure}
\subparagraph{}
如何判断此时的算法处于哪个状态？
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{620.png}}
\label{fig:620}
\caption{Diagnosing Bias/variance}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{621.png}}
\label{fig:621}
\end{figure}
\subparagraph{}
In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.
\begin{itemize}
\item We need to distinguish whether bias or variance is the problem contributing to bad predictions.
\item High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.
\end{itemize}
\subparagraph{}
The training error will tend to decrease as we increase the degree d of the polynomial.
\subparagraph{}
At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve.
\subparagraph{}
High bias (underfitting): both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta)\approx{J_{train}(\Theta)}$.
\subparagraph{}
High variance (overfitting): $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$.
\subparagraph{}
The is summarized in the figure below:
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{625.png}}
\label{fig:625}
\caption{Diagnosing Bias/variance}
\end{figure}
\subsection{Regularization and Bias/Variance}
\subparagraph{}
算法正则化与方差偏差的关系：
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{627.png}}
\label{fig:627}
\caption{线性回归与正则化}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{628.png}}
\label{fig:628}
\end{figure}
\subparagraph{}
In the figure above, we see that as λ increases, our fit becomes more rigid. On the other hand, as λ approaches 0, we tend to over overfit the data. So how do we choose our parameter λ to get it 'just right' ? In order to choose the model and the regularization term λ, we need to:
\subparagraph{}
1.Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});
\subparagraph{}
2.Create a set of models with different degrees or any other variants.
\subparagraph{}
3.Iterate through the λs and for each λ go through all the models to learn some $\Theta$.
\subparagraph{}
4.Compute the cross validation error using the learned $\Theta$ (computed with λ) on the $J_{CV}(\Theta)$ without regularization or λ = 0.
\subparagraph{}
5.Select the best combo that produces the lowest error on the cross validation set.
\subparagraph{}
6.Using the best combo $\Theta$ and λ, apply it on $J_{test}(\Theta)$ to see if it has a good generalization of the problem.
\subparagraph{}
当改变$\lambda$时，交叉验证集误差和训练集误差会发生怎样的变化？
\subparagraph{}
我们选择一系列的想要测试的 λ 值,通常是 0-10 之间的呈现 2 倍关系的值(如:
0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10 共 12 个)。 我们同样把数据分为训练
集、交叉验证集和测试集。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{630.png}}
\label{fig:630}
\caption{选择合适的正则化参数$\lambda$}
\end{figure}
\subparagraph{}
选择$\lambda$的方法是：
\subparagraph{}
1. 使用训练集训练出 12 个不同程度正则化的模型
\subparagraph{}
2. 用 12 模型分别对交叉验证集计算的出交叉验证误差
\subparagraph{}
3. 选择得出交叉验证误差最小的模型
\subparagraph{}
4. 运用步骤 3 中选出模型对测试集计算得出推广误差,我们也可以同时将训练集和交叉验证集模型的代价函数误差与 λ 的值绘制在一张图表上:
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{633.png}}
\label{fig:633}
\caption{正则化参数$\lambda$与偏差和方差}
\end{figure}
\subparagraph{}
\begin{itemize}
\item 当 λ 较小时,训练集误差较小(过拟合)而交叉验证集误差较大
\item 随着 λ 的增加,训练集误差不断增加(欠拟合),而交叉验证集误差则是先减小后
增加
\end{itemize}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{631.png}}
\label{fig:631}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.2\textwidth]{632.png}}
\label{fig:632}
\end{figure}
\subsection{Learning Curves}
\subparagraph{}
学习曲线就是一种很好的工具,我经常使用学习曲线来判断某一个学习算法是否处于偏
差、方差问题。学习曲线是学习算法的一个很好的合理检验(sanity check)。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量(m)的函数绘制的图表。
\subparagraph{}
即,如果我们有 100 行数据,我们从 1 行数据开始,逐渐学习更多行的数据。
\subparagraph{}
思想是:当训练较少行数据的时候,训练的模型将能够非常完美地适应较少的训练数据,但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{635.png}}
\label{fig:635}
\caption{学习曲线}
\end{figure}
\subparagraph{}
Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:
\subparagraph{}
1.As the training set gets larger, the error for a quadratic function increases.
\subparagraph{}
2.The error value will plateau out after a certain m, or training set size.
\subparagraph{}
1.训练集误差随着m的增大而增大
\subparagraph{}
2.当训练集较小时，泛化程度不会很好，不能很好的适应新样本，因此这个假设不是一个理想的假设，只有当我使用一个更大的训练集时，才有可能得到一个能够更好拟合数据的 可能的假设，因此验证集误差和测试集误差都会随着训练集样本容量m的增加而减小，因为使用的数据越，越能获得更好地泛化表现，或者说对新样本的适应能力更强，因此数据越多 越能拟合出合适的假设。
\subparagraph{}
当处于高方差或者高偏差的情况时这些曲线又会变成什么样子？
\subsubsection{High bias}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{636.png}}
\label{fig:636}
\caption{高偏差学习曲线}
\end{figure}
\subparagraph{}
最左端表示训练集样本容量很小，比如说只有一组样本，那么表现当然很不好，而随着你增大训练集样本数，当达到某一个容量值的时候 你就会找到那条最有可能拟合数据的那条线，并且此时即便你继续增大训练集的样本容量， 即使你不断增大m的值，你基本上还是会得到的一条差不多的直线，因此交叉验证集误差 我把它标在这里，或者测试集误差，将会很快变为水平而不再变化。只要训练集样本容量值达到或超过了那个特定的数值，交叉验证集误差和测试集误差就趋于不变，这样你会得到最能拟合数据的那条直线；
\subparagraph{}
那么，训练误差又如何呢？
\subparagraph{}
在高偏差的情形中，你会发现训练集误差会逐渐增大，一直趋于接近交叉验证集误差，这是因为你的参数很少，但当m很大的时候，数据太多，此时训练集和交叉验证集的预测效果将会非常接近。
\subparagraph{}
高偏差的情形反映出的问题是,交叉验证集和训练集误差都很大,也就是说,你最终会得到一个值比较大$J_{CV}(\Theta)$和$J_{train}(\Theta)$。
\subparagraph{}
Low training set size: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.
\subparagraph{}
Large training set size: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta)\approx{J_{CV}(\Theta)}$.
\subparagraph{}
If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{640.png}}
\label{fig:640}
\caption{高偏差学习曲线}
\end{figure}
\subsubsection{High variance}
\subparagraph{}
Low training set size: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.
\subparagraph{}
Large training set size: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta)<{J_{CV}(\Theta)}$ but the difference between them remains significant.
\subparagraph{}
If a learning algorithm is suffering from high variance, getting more training data is likely to help.
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{641.png}}
\label{fig:641}
\caption{高方差学习曲线1}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{637.png}}
\label{fig:637}
\caption{高方差学习曲线2}
\end{figure}
\subparagraph{}
如果继续增大训练样本的数量，将曲线向右延伸，交叉验证集误差将会逐渐下降，所以在高方差的情形中，使用更多的训练集数据对改进算法的表现，事实上是有效果的，这同样也体现出，知道你的算法正处于高方差的情形也是非常有意义的，因为它能告诉你，是否有必要花时间来增加更多的训练集数据。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{638.png}}
\label{fig:638}
\end{figure}
\subsection{Deciding What to Do Next Revisited}
\subparagraph{}
我们已经介绍了怎样评价一个学习算法,我们讨论了模型选择问题,偏差和方差的问题。
那么这些诊断法则怎样帮助我们判断,哪些方法可能有助于改进学习算法的效果,而哪些可
能是徒劳的呢?
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{643.png}}
\label{fig:643}
\end{figure}
让我们再次回到最开始的例子,在那里寻找答案,这就是我们之前的例子。回顾之前所学的
提出的六种可选的下一步,让我们来看一看我们在什么情况下应该怎样选择:
\subparagraph{}
1.获得更多的训练实例——解决高方差
\subparagraph{}
2. 尝试减少特征的数量——解决高方差
\subparagraph{}
3. 尝试获得更多的特征——解决高偏差
\subparagraph{}
4. 尝试增加多项式特征——解决高偏差
\subparagraph{}
5. 尝试减少正则化程度 λ——解决高偏差
\subparagraph{}
6. 尝试增加正则化程度 λ——解决高方差
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{649.png}}
\label{fig:649}
\caption{诊断神经网络}
\end{figure}
\subparagraph{}
A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper.
\subparagraph{}
A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting.
\subparagraph{}
一般来说,使用一个大型的神经网络并使用正则化来修正过拟合问题,通常比使用一个小型的神经网络效果更好,但主要可能出现的问题 是计算量相对较大,最后还需要选择隐藏层的层数，你是应该用一个隐藏层呢，还是应该用三个呢，就像我们这里画的，或者还是用两个隐藏层呢 ？
\subparagraph{}
Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. 
\subparagraph{}
通常来说,正如我在前面的视频中讲过的,默认的情况是 ,使用一个隐藏层 ,但是如果你确实想要选择多个隐藏层 ,你也可以试试把数据分割为训练集, 验证集和测试集 ,然后使用交叉验证的方法 ,比较一个隐藏层的神经网络 ,然后试试两个 ,三个隐藏层 ,以此类推 ,然后看看哪个神经网络 ,在交叉验证集上表现得最理想 ,也就是说 ,你得到了三个神经网络模型 ,分别有一个 ,两个, 三个隐藏层 ,然后你对每一个模型都用交叉验证集数据进行测试 ,算出三种情况下的 ,交叉验证集误差$J_{CV}(\Theta)$然后选出你认为最好的神经网络结构。
\subparagraph{}
这就是偏差和方差问题以及诊断该问题的学习曲线方法，在改进学习算法的表现时 ，你可以充分运用以上这些内容来判断哪些途径可能是有帮助的，而哪些方法可能是无意义的。 
\subparagraph{}
Model Complexity Effects:
\begin{itemize}
\item Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.
\item Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.
\item In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.
\end{itemize}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{650.png}}
\label{fig:650}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{651.png}}
\label{fig:651}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{652.png}}
\label{fig:652}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{653.png}}
\label{fig:653}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{654.png}}
\label{fig:654}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{655.png}}
\label{fig:655}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{656.png}}
\label{fig:656}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{657.png}}
\label{fig:657}
\caption{此题有误}
\end{figure}
\section{Building a Spam Classifier}
\subsection{Prioritizing What to Work On}
\subparagraph{}
首先要做什么，在接下来的视频中,我将谈到机器学习系统的设计。这些视频将谈及在设计复杂的机器学习系统时,你将遇到的主要问题。同时我们会试着给出一些关于如何巧妙构建一个复杂的机器学习系统的建议。下面的课程的的数学性可能不是那么强,但是我认为我们将要讲到的这些东西是非常有用的,可能在构建大型的机器学习系统时,节省大量的时间。
\subparagraph{}
Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.
\subparagraph{}
以一个垃圾邮件分类器算法为例进行讨论。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{660.png}}
\label{fig:660}
\caption{垃圾邮件分类器}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{661.png}}
\label{fig:661}
\caption{建立分类器}
\end{figure}
\subparagraph{}
为了解决这样一个问题,我们首先要做的决定是如何选择并表达特征向量 x。我们可以
选择一个由 100 个最常出现在垃圾邮件中的词所构成的列表,根据这些词是否有在邮件中
出现,来获得我们的特征向量(出现为 1,不出现为 0),尺寸为 100×1。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{662.png}}
\label{fig:662}
\caption{构建分类器}
\end{figure}
为了构建这个分类器算法,我们可以做很多事,例如:
\subparagraph{}
1. 收集更多的数据,让我们有更多的垃圾邮件和非垃圾邮件的样本
\subparagraph{}
2. 基于邮件的路由信息开发一系列复杂的特征
\subparagraph{}
3. 基于邮件的正文信息开发一系列复杂的特征,包括考虑截词的处理
\subparagraph{}
4. 为探测刻意的拼写错误(把 watch 写成 w4tch)开发复杂的算法
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{671.png}}
\label{fig:671}
\end{figure}
It is difficult to tell which of the options will be most helpful.
\begin{figure}[H]
\center{\includegraphics[width=.9\textwidth]{670.png}}
\label{fig:670}
\end{figure}
\subparagraph{}
在上面这些选项中,非常难决定应该在哪一项上花费时间和精力,作出明智的选择,比
随着感觉走要更好。当我们使用机器学习时,总是可以“头脑风暴”一下,想出一堆方法来
试试。实际上,当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候,你可能已
经超越了很多人了。大部分人并不尝试着列出可能的方法,他们做的只是某天早上醒来,因
为某些原因有了一个突发奇想:"让我们来试试用 Honey Pot 项目收集大量的数据吧。"
\subparagraph{}
我们将在随后的课程中讲误差分析,我会告诉你怎样用一个更加系统性的方法,从一堆
不同的方法中,选取合适的那一个。因此,你更有可能选择一个真正的好方法,能让你花上
几天几周,甚至是几个月去进行深入的研究。
\subsection{Error Analysis}
\subparagraph{}
在本次课程中,将会讲到误差分析(error analysis)的概念。这会帮助你更系统地做出决定。如果准备研究机器学习的东西,或者构造机器学习应用程序,最好的实践方法
不是建立一个非常复杂的系统,拥有多么复杂的变量,而是构建一个简单的算法,这样你可
以很快地实现它.
\subparagraph{}
每当我研究机器学习的问题时,我最多只会花一天的时间,就是字面意义上的 24 小时,
来试图很快的把结果搞出来,即便效果不好。坦白的说,就是根本没有用复杂的系统,但是
只是很快的得到的结果。即便运行得不完美,但是也把它运行一遍,最后通过交叉验证来检
验数据。一旦做完,你可以画出学习曲线,通过画出学习曲线,以及检验误差,来找出你的
算法是否有高偏差和高方差的问题,或者别的问题。在这样分析之后,再来决定用更多的数
据训练,或者加入更多的特征变量是否有用。
\subparagraph{}
这么做的原因是:这在你刚接触机器学习问题
时是一个很好的方法,你并不能提前知道你是否需要复杂的特征变量,或者你是否需要更多
的数据,还是别的什么。提前知道你应该做什么,是非常难的,因为你缺少证据,缺少学习
曲线。因此,你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个
非常简单即便不完美的方法时,你可以通过画出学习曲线来做出进一步的选择。你可以用这
种方式来避免一种电脑编程里的过早优化问题,这种理念是:我们必须用证据来领导我们的决策,怎样分配自己的时间来优化算法,而不是仅仅凭直觉,凭直觉得出的东西一般总是错误的。除了画出学习曲线之外,一件非常有用的事是误差分析,我的意思是说:当我们在构造垃圾邮件分类器时,我会看一看我的交叉验证数据集,然后亲自看一看哪些邮件被算法错误地分类。
\subparagraph{}
因此,通过这些被算法错误分类的垃圾邮件与非垃圾邮件,你可以发现某些系统性的规律:什么类型的邮件总是被错误分类。经常地这样做之后,这个过程能启发你构造新的特征变量,或者告诉你:现在这个系统的短处,然后启发你如何去提高它。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{674.png}}
\label{fig:674}
\end{figure}
\subparagraph{}
构建一个学习算法的推荐方法为:
\subparagraph{}
1. 从一个简单的能快速实现的算法开始,实现该算法并用交叉验证集数据测试这个算法
\subparagraph{}
Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.
\subparagraph{}
2. 绘制学习曲线,决定是增加更多数据,或者添加更多特征,还是其他选择
\subparagraph{}
Plot learning curves to decide if more data, more features, etc. are likely to help.
\subparagraph{}
3. 进行误差分析:人工检查交叉验证集中我们算法中产生预测误差的实例,看看这些实例是否有某种系统化的趋势
\subparagraph{}
Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{675.png}}
\label{fig:675}
\caption{误差分析}
\end{figure}
For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate:
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{673.png}}
\label{fig:673}
\caption{数值计算方式评估机器学习算法}
\end{figure}
\subparagraph{}
It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm's performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3\%{} error rate instead of 5\%{}, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2\%{} error rate instead of 3\%{}, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. 
\subparagraph{}
以我们的垃圾邮件过滤器为例,误差分析要做的既是检验交叉验证集中我们的算法产生
错误预测的所有邮件,看:是否能将这些邮件按照类分组。例如医药品垃圾邮件,仿冒品垃
圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大,并着手优化。
\subparagraph{}
思考怎样能改进分类器。例如,发现是否缺少某些特征,记下这些特征出现的次数。
\subparagraph{}
例如记录下错误拼写出现了多少次,异常的邮件路由情况出现了多少次等等,然后从出
现次数最多的情况开始着手优化。
\subparagraph{}
误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型,
然后进行比较,在模型比较时,用数值来判断哪一个模型更好更有效,通常我们是看交叉验
证集的误差。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{681.png}}
\label{fig:681}
\end{figure}
\subparagraph{}
在我们的垃圾邮件分类器例子中,对于“我们 是否应该将discount/discounts/discounted/discounting 处理成同一个词?”如果这样做可以改善我们算法,我们会采用一些截词软件。误差分析不能帮助我们做出这类判断,我们只能尝试采用和采用截词软件这两种不同方案,然后根据数值检验的结果来判断哪一种更好。
\subparagraph{}
因此,当你在构造学习算法的时候,你总是会去尝试很多新的想法,实现出很多版本的学习算法,如果每一次你实践新想法的时候,你都要手动地检测这些例子,去看看是表现差
还是表现好,那么这很难让你做出决定。到底是否使用词干提取,是否区分大小写。但是通
过一个量化的数值评估,你可以看看这个数字,误差是变大还是变小了。你可以通过它更快地实践你的新想法,它基本上非常直观地告诉你:你的想法是提高了算法表现,还是让它变得更坏,这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析,而不是在测试集上。但是,还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{680.png}}
\label{fig:680}
\end{figure}
\subparagraph{}
总结一下,当你在研究一个新的机器学习问题时,我总是推荐你实现一个较为简单快速、
即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是:花费大量的
时间在构造算法上,构造他们以为的简单的方法。因此,不要担心你的算法太简单,或者太
不完美,而是尽可能快地实现你的算法。当你有了初始的实现之后,它会变成一个非常有力
的工具,来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误,通过误差分析,来看看他犯了什么错,然后来决定优化的方式。另一件事是:假设你有了一个快速而不完美的算法实现,又有一个数值的评估数据,这会帮助你尝试新的想法,快速地发现你尝试的这些想法是否能够提高算法的表现,从而你会更快地做出决定,在算法中放弃什么,吸收什么,误差分析可以帮助我们系统化地选择该做什么。
\section{Handing Skewed Data}
\subsection{Error Metrics for Skewed Classes}
\subparagraph{}
在前面的课程中,我提到了误差分析,以及设定误差度量值的重要性。那就是,设定某
个实数来评估你的学习算法,并衡量它的表现,有了算法的评估和误差度量值。
\subparagraph{}
有一件重要的事情要注意,就是使用一个合适的误差度量值,这有时会对于你的学习算法造成非常微妙的影响,这件重要的事情就是偏斜类(skewed classes)的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的实例,只有很少或没有其他类的实例。
\subparagraph{}
例如我们希望用算法来预测癌症是否是恶性的,在我们的训练集中,只有 0.5%的实例
是恶性肿瘤。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{685.png}}
\label{fig:685}
\caption{癌症分类举例}
\end{figure}
\subparagraph{}
假设我们编写一个非学习而来的算法,在所有情况下都预测肿瘤是良性的,那么误差只有 0.5\%{}。然而我们通过训练而得到的神经网络算法却有 1\%{}的误差。这时,误差的大小是不能视为评判算法效果的依据的。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{684.png}}
\label{fig:684}
\caption{Precision/Recall}
\end{figure}
查准率(Precision)和查全率（召回率）(Recall) 我们将算法预测的结果分成四种情况:
\subparagraph{}
1. 正确肯定(True Positive,TP):预测为真,实际为真
\subparagraph{}
2. 正确否定(True Negative,TN):预测为假,实际为假
\subparagraph{}
3. 错误肯定(False Positive,FP):预测为真,实际为假
\subparagraph{}
4. 错误否定(False Negative,FN):预测为假,实际为真
\subparagraph{}
则：
\subparagraph{}
查准率=TP/(TP+FP)例,在所有我们预测有恶性肿瘤的病人中,实际上有恶性肿瘤的病
人的百分比,越高越好。
\subparagraph{}
对于那些病人,我们告诉他们"你们患有癌症" 对于这些病人而言,有多大比率是真正患有癌症的 ,这个就叫做查准率 ,另一个写法是 分子是真阳性 ,分母是 ,所有预测阳性的数量 那么这个等于 ,表格第一行的值的和,也就是真阳性除以真阳性,这里我把阳性简写为 POS 加上假阳性 这里我还是把阳性简写为POS。
\subparagraph{}
查全率=TP/(TP+FN)例,在所有实际上有恶性肿瘤的病人中,成功预测有恶性肿瘤的病人的百分比,越高越好。
\subparagraph{}
召回率是如果所有的病人假设测试集中的病人 或者交叉验证集中的,如果所有这些在数据集中的病人确实得了癌症,有多大比率,我们正确预测他们得了癌症,如果所有的病人都患了癌症,有多少人我们能够正确告诉他们你需要治疗。
\subparagraph{}
我们用这个来除以实际阳性,这个值是 所有患有癌症的人的数量有多大比率我们能正确发现癌症并给予治疗,把这个以另一种形式 写下来,分母是实际阳性的数量,表格第一列值的和,将这个以不同的形式写下来,那就是,真阳性除以真阳性加上假阴性,同样地召回率越高越好,通过计算查准率和召回率,我们能更好的知道分类模型到底好不好。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{686.png}}
\label{fig:686}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{687.png}}
\label{fig:687}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{688.png}}
\label{fig:688}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{689.png}}
\label{fig:689}
\end{figure}
\subparagraph{}
这样,对于我们刚才那个总是预测病人肿瘤为良性的算法,其查全率是0。
\subparagraph{}
在查准率和召回率的定义中,我们定义,查准率和召回率, 我们总是习惯性地用y=1, 如果这个类出现得非常少, 因此如果我们试图检测 ,某种很稀少的情况, 比如癌症 ,我希望它是个很稀少的情况, 查准率和召回率会被定义为 y=1而不是y=0,作为某种我们希望检测的出现较少的类 ,通过使用查准率和召回率,我们发现即使我们拥有非常偏斜的类 ,算法不能够通过总是预测y=1来"欺骗"我们 或者总是预测y=0,因为它不能够获得高查准率和召回率 ,具体地说 ,如果一个分类模型 拥有高查准率和召回率 ,那么我们可以确信地说这个算法表现很好,即便我们拥有很偏斜的类。
\subparagraph{}
对于偏斜类的问题,查准率和召回率给予了我们更好的方法来检测学习算法表现如何,这是一种更好地评估学习算法的标准,当出现偏斜类时比仅仅只用分类误差或者分类精度好。 
\subsection{Trading Off Precision and Recall}
\subparagraph{}
在之前的课程中,我们谈到查准率和召回率,作为遇到偏斜类问题的评估度量值。在很多应用中,我们希望能够保证查准率和召回率的相对平衡。
\subparagraph{}
在这节课中,我将告诉你应该怎么做,同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。
\subparagraph{}
继续沿用刚才预测肿瘤性质的例子。假使,我们的算法输出的结果在 0-1 之间,我们使用阀值 0.5 来预测真和假。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{690.png}}
\label{fig:690}
\caption{查准率与召回率的权衡}
\end{figure}
\subparagraph{}
查准率(Precision)=TP/(TP+FP) 例,在所有我们预测有恶性肿瘤的病人中,实际上
有恶性肿瘤的病人的百分比,越高越好。
\subparagraph{}
查全率(Recall)=TP/(TP+FN)例,在所有实际上有恶性肿瘤的病人中,成功预测有恶
性肿瘤的病人的百分比,越高越好。
\subparagraph{}
如果我们希望只在非常确信的情况下预测为真(肿瘤为恶性),即我们希望更高的查准率,我们可以使用比 0.5 更大的阀值,如0.7,0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况,同时却会增加未能成功预测肿瘤为恶性的情况。
\subparagraph{}
如果我们希望提高查全率,尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检
查、诊断,我们可以使用比 0.5 更小的阀值,如 0.3。
\subparagraph{}
我们可以将不同阀值情况下,查全率与查准率的关系绘制成图表,曲线的形状根据数据
的不同而不同:
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{691.png}}
\label{fig:691}
\caption{查准率与召回率的关系曲线}
\end{figure}
\subparagraph{}
我们有没有自动选择临界值的办法，如何比较不同的查准率和召回率。
\subparagraph{}
假设有三个不同的学习算法，算法是一样的，但是临界值不一样，我们怎样决定哪一个算法是最好的。
\subparagraph{}
我们之前讲到的其中一件事就是，评估度量值的重要性 ，这个概念是通过一个具体的数字 来反映你的回归模型到底如何，但是查准率和召回率的问题， 我们却不能这样做， 因为在这里我们有两个可以判断的数字。 
\subparagraph{}
如果我们有一个评估度量值 ，一个数字 ，能够告诉我们到底是算法1好还是算法2好 ，这能够帮助我们更快地决定 ，哪一个算法更好 ，同时也能够更快地帮助我们评估不同的改动， 哪些应该被融入进算法里面 ，那么我们怎样才能得到这个评估度量值呢。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{694.png}}
\label{fig:694}
\caption{$F_1Score$}
\end{figure}
\subparagraph{}
我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算$F_1$值($F_1$ Score),其计算公式为:
\begin{equation}
F_1Score=2\frac{PR}{P+R}
\end{equation}
\subparagraph{}
选择使得$F_1$最高的阈值。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{692.png}}
\label{fig:692}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{693.png}}
\label{fig:693}
\end{figure}
\section{Using Large Data Sets}
\subsection{Data For Machine Learning}
\subparagraph{}
在之前的视频中,我们讨论了评价指标。在这个视频中,我要稍微转换一下,讨论一下
机器学习系统设计中另一个重要的方面,这往往涉及到用来训练的数据有多少。在之前的一
些视频中,我曾告诫大家不要盲目地开始,而是花大量的时间来收集大量的数据,因为数据
有时是唯一能实际起到作用的。但事实证明,在一定条件下,我会在这个视频里讲到这些条
件是什么。
\subparagraph{}
得到大量的数据并在某种类型的学习算法中进行训练,可以是一种有效的方法来获得一个具有良好性能的学习算法。而这种情况往往出现在这些条件对于你的问题都成立,并且你能够得到大量数据的情况下。这可以是一个很好的方式来获得非常高性能的学习算法。因此,在这段视频中,让我们一起讨论一下这个问题。
\subparagraph{}
很多很多年前,我认识的两位研究人员 Michele Banko 和 Eric Brill 进行了一项有趣的研究,他们尝试通过机器学习算法来区分常见的易混淆的单词,他们尝试了许多种不同的算法,并发现数据量非常大时,这些不同类型的算法效果都很好。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{695.png}}
\label{fig:695}
\end{figure}
\subparagraph{}
比如,在这样的句子中:早餐我吃了\_{}\_{}个鸡蛋 (to,two,too),在这个例子中,“早餐我吃
了 2 个鸡蛋”,这是一个易混淆的单词的例子。于是他们把诸如这样的机器学习问题,当做一类监督学习问题,并尝试将其分类,什么样的词,在一个英文句子特定的位置,才是合适的。他们用了几种不同的学习算法,这些算法都是在他们 2001 年进行研究的时候,都已经被公认是比较领先的。因此他们使用了一个方差,用于逻辑回归上的一个方差,被称作"感知器" (perceptron)。他们也采取了一些过去常用,但是现在比较少用的算法,比如 Winnow算法,很类似于回归问题,但在一些方面又有所不同,过去用得比较多,但现在用得不太多。还有一种基于内存的学习算法,现在也用得比较少了,但是我稍后会讨论一点,而且他们用了一个朴素算法。这些具体算法的细节不那么重要,我们下面希望探讨,什么时候我们会希望获得更多数据,而非修改算法。他们所做的就是改变了训练数据集的大小,并尝试将这些学习算法用于不同大小的训练数据集中,这就是他们得到的结果。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{696.png}}
\label{fig:696}
\end{figure}
\subparagraph{}
这些趋势非常明显首先大部分算法,都具有相似的性能,其次,随着训练数据集的增大,
在横轴上代表以百万为单位的训练集大小,从 0.1 个百万到 1000 百万,也就是到了 10 亿规模的训练集的样本,这些算法的性能也都对应地增强了。
\subparagraph{}
事实上,如果你选择任意一个算法,可能是选择了一个"劣等的"算法,如果你给这个劣
等算法更多的数据,那么从这些例子中看起来的话,它看上去很有可能会其他算法更好,甚
至会比"优等算法"更好。由于这项原始的研究非常具有影响力,因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明,许多不同的学习算法有时倾向于表现出非常相似的表现,这还取决于一些细节,但是真正能提高性能的,是你能够给一个算法大量的训练数据。像这样的结果,引起了一种在机器学习中的普遍共识:"取得成功的人不是拥有最好算法的人,而是拥有最多数据的人"。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{697.png}}
\label{fig:697}
\end{figure}
\subparagraph{}
那么这种说法在什么时候是真,什么时候是假呢?因为如果我们有一个学习算法,并且
如果这种说法是真的,那么得到大量的数据通常是保证我们具有一个高性能算法的最佳方
式, 而不是去争辩应该用什么样的算法。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{698.png}}
\label{fig:698}
\end{figure}
\subparagraph{}
假如有这样一些假设,在这些假设下有大量我们认为有用的训练集,我们假设在我们的
机器学习问题中,特征值 x 包含了足够的信息,这些信息可以帮助我们用来准确地预测 y,例如,如果我们采用了一些容易混淆的词,如:two、to、too,假如说它能够描述 x,捕捉到需要填写的空白处周围的词语,那么特征捕捉到之后,我们就希望有 对于“早饭我吃了\_{}\_{}鸡蛋”,那么这就有大量的信息来告诉我中间我需要填的词是“两个” (two),而不是单词 to 或 too,因此特征捕捉,哪怕是周围词语中的一个词,就能够给我足够的信息来确定出标签 y 是什么。换句话说,从这三组易混淆的词中,我应该选什么词来填空。
\subparagraph{}
给定一个输入特征向量 x, 给定这些特征值, 也给定了相同的可用的信息和学习算法 ,
如果我们去请教这个领域的人类专家, 一个人类专家能够准确或自信地预测出 y 的值吗？ 第一个例子 ,如果我们去找你认识的一个英语专家, 比如你找到了一个英语说得很好的人 ,那么 ,一个英语方面的专家 ,大部分像你和我这样的人, 我们可能不难预测出在这种情况下, 该使用什么样的语言, 一个英语说得好的人应该可以预测得很好, 因此这就给了我信心, x能够让我们准确地预测y. 但是与此相反 ,如果我们去找一个价格上的专家, 比如 ,可能是一个房地产经纪人 ,或者职业售楼小姐 ,如果我只是告诉他们, 一个房子的大小 ,然后问他们房子的价格 ,那么即使是擅长房价评估, 或者售房方面的专家 ,也不能告诉我, 房子的价格是多少 ,所以在房价的例子中 ,只知道房子的大小并不能给我足够的信息来预测,房子的价格。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6666.png}}
\label{fig:6666}
\end{figure}
\subparagraph{}
如果这个假设是成立的，那么让我们来看一看,大量的数据是有帮助的情况。假设特征值有足够的信息来预测 y值,假设我们使用一种需要大量参数的学习算法,比如有很多特征的逻辑回归或线性回归,或者用带有许多隐藏单元的神经网络,那又是另外一种带有很多参数的学习算法,这些都是非常强大的学习算法,它们有很多参数,这些参数可以拟合非常复杂的函数,因此我要调用这些,我将把这些算法想象成低偏差算法,因为我们能够拟合非常复杂的函数,而且因为我们有非常强大的学习算法,这些学习算法能够拟合非常复杂的函数。很有可能,如果我们用这些数据运行这些算法,这种算法能很好地拟合训练集,因此,训练误差就会很低了。
\subparagraph{}
现在假设我们使用了非常非常大的训练集,在这种情况下,尽管我们希望有很多参数,
但是如果训练集比参数的数量还大,甚至是更多,那么这些算法就不太可能会过度拟合。也
就是说训练误差有希望接近测试误差。
\subparagraph{}
另一种考虑这个问题的角度是为了有一个高性能的学习算法,我们希望它不要有高的偏
差和方差。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{699.png}}
\label{fig:699}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{700.png}}
\label{fig:700}
\end{figure}
\subparagraph{}
因此偏差问题,我么将通过确保有一个具有很多参数的学习算法来解决,以便我们能够得到一个较低偏差的算法,并且通过用非常大的训练集来保证。
\subparagraph{}
我们在此没有方差问题,我们的算法将没有方差,并且通过将这两个值放在一起,我们
最终可以得到一个低误差和低方差的学习算法。这使得我们能够很好地测试测试数据集。从根本上来说,这是一个关键的假设:特征值有足够的信息量,且我们有一类很好的函数,这是为什么能保证低误差的关键所在。它有大量的训练数据集,这能保证得到更多的方差值,因此这给我们提出了一些可能的条件,如果你有大量的数据,而且你训练了一种带有很多参数的学习算法,那么这将会是一个很好的方式,来提供一个高性能的学习算法。
\subparagraph{}
我觉得关键的测试:首先,一个人类专家看到了特征值 x,能很有信心的预测出 y 值吗?
因为这可以证明 y 可以根据特征值 x 被准确地预测出来。其次,我们实际上能得到一组庞大的训练集,并且在这个训练集中训练一个有很多参数的学习算法吗?如果你不能做到这两者,那么更多时候,你会得到一个性能很好的学习算法。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6667.png}}
\label{fig:6667}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6668.png}}
\label{fig:6668}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6674.png}}
\label{fig:6669}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6670.png}}
\label{fig:6670}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6671.png}}
\label{fig:6671}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6672.png}}
\label{fig:6672}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6673.png}}
\label{fig:6673}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{6677.png}}
\label{fig:6677}
\end{figure}
\end{CJK}
\end{document}