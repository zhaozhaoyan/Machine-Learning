\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 5}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\section{Cost Function and Backpropagation}
\subsection{Cost Function}
\subparagraph{}
首先引入一些便于稍后讨论的新标记方法：
\subparagraph{}
假设神经网络的训练样本有m个，每个包含一组输入x和输出信号y，L表示神经网络层数，$s_l$表示每层的neuron的个数，$s_L$表示最后一层中处理单元的个数。
\subparagraph{}
Let's first define a few variables that we will need to use:
\begin{itemize}
 \item L = total number of layers in the network
 \item $s_l$ = number of units (not counting bias unit) in layer l
 \item K = number of output units/classes
\end{itemize}
\subparagraph{}
将神经网络的分类定义为两种情况：二类分类和多类分类。
\subparagraph{}
二类分类（Binary classification）:
\begin{itemize}
  \item $s_L=1$，y= 0 or 1 ,表示哪一类；
  \item 1 output unit
\end{itemize}
\subparagraph{}
K类分类（Multi-class classification（K classes））：
\begin{itemize}
  \item $s_L=K$,$y_i=1$，表示分到第i类（$K\geqslant3$）;
  \item K output units
\end{itemize}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{502.png}}
\caption{二类分类和K类分类}
\label{fig:502}
\end{figure}
\subparagraph{}
Recall that in neural networks, we may have many output nodes. We denote $h_\Theta{(x)}_k$ as being a hypothesis that results in the kth output. Our cost function for neural networks is going to be a generalization of the one we used for logistic regression.
\subparagraph{}
Recall that the cost function for regularized logistic regression was:
\begin{equation}
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{equation}
\subparagraph{}
在逻辑回归中，只有一个输出变量，又称标量（scalar），也只有一个因变量y，但是在神经网络中，可以有很多输出变量，$h_\Theta(x)$是一个K维度的向量，并且训练集中的因变量也是同样维度的一个向量，因此神经网络的代价函数会比逻辑回归更加复杂一些。
\subparagraph{}
For Neural Networks:
\begin{equation}
J(\Theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K[y_k^{(i)}log((h_\Theta(x^{(i)}))_k)+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{j,i}^{(l)})^2
\end{equation}
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{503.png}}
\caption{逻辑回归和神经网络的代价函数}
\label{fig:503}
\end{figure}
\subparagraph{}
这个看起来复杂很多的代价函数背后的思想还是一样的，希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出K个预测，基本上可以利用循环，对每一行特征都预测K个不同的结果，然后利用循环在K个预测中选择可能性最高的一个，将其与y中的实际数据进行比较。
\subparagraph{}
正则化的那一项只是排除了每一层的$\Theta_0$后，每一层的$\Theta$矩阵的和，最里层的循环j循环所有的行（由$s_l+1$层的激活单元数决定），循环i则循环所有的列，由该层（$s_l$层）的激活单元数所决定。即：$h_\Theta(x)$与真实值之间的距离为每个样本，每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。
\subparagraph{}
We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.
\subparagraph{}
In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.
\subparagraph{}
Note:
\begin{itemize}
\item the double sum simply adds up the logistic regression costs calculated for each cell in the output layer
\item the triple sum simply adds up the squares of all the individual Θs in the entire network.
\item the i in the triple sum does not refer to training example i
\end{itemize}
\subsection{Backpropagation Algorithm}
\subparagraph{}
之前在计算神经网络预测结果时采用的是一种正向传播方法，从第一层开始正向一层一层进行计算，直到最后一层的$h_\Theta(x)$。
\subparagraph{}
现在，为了计算代价函数的偏导数$\frac{\partial}{\partial\Theta_{i,j}^{(l)}}J(\Theta)$，我们采用一种反向传播算法，也就是首先计算一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。
\subparagraph{}
举例说明反向传播算法：
\subparagraph{}
假设我们的训练集只有一个实例（$x^{(1)}$,$y^{(1)}$），我们的神经网络是一个四层的神经网络，其中 K=4，$S_L=4$，L=4：
\subparagraph{}
前向传播算法：
\begin{align}
a^{(1)} & =x \\
z^{(2)} & =\Theta^{(1)}a^{(1)} \\
a^{(2)} & =g(z^{(2)})(add\quad{a_0^{(2)}}) \\
z^{(3)} & =\Theta^{(2)}a^{(2)} \\
a^{(3)} & =g(z^{(3)})(add\quad{a_0^{(3)}}) \\
z^{(4)} & =\Theta^{(3)}a^{(3)} \\
a^{(4)} & =h_\Theta(x)=g(z^{(4)})
\end{align}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{504.png}}
\caption{前向传播算法求激励函数}
\label{fig:504}
\end{figure}
计算偏导数项利用反向传播算法：Backpropagation Algorithm
\subparagraph{}
Intuition:$\delta_j^{(i)}$="error" of node j in layer l.
\subparagraph{}
$a_j^{(i)}$:第l层第j个单元（节点）的激励值。
\subparagraph{}
我们从最后一层的误差开始计算，误差是激活单元的预测（$a_j^{(4)}$）与实际值（$y_j$）之间的误差，j=1:L。
\subparagraph{}
For each output unit(layer L=4):
\begin{equation}
\delta_j^{(4)}=a_j^{(4)}-y_j
\end{equation}
\subparagraph{}
这里$\delta_j^{(4)}$相当于$(h_\Theta(x))_j$
\subparagraph{}
我们用$\delta$来表示误差，则（向量化）：
\begin{equation}
\delta^{(4)}=a^{(4)}-y
\end{equation}
\subsection{Backpropagation Intuition}
\subparagraph{}
\section{Backpropagation in Practice}
\subsection{Implementation Note:Unrolling Parameters}
\subsection{Gradient Checking}
\subsection{Random Initialization}
\paragraph{ }
任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0,这样的初始方法对于逻辑回归来说是可行的,但是对于神经网络来说是不可行的。如果我们令所有
的初始参数都为 0,这将意味着我们第二层的所有激活单元都会有相同的值。同理,如果我们初始所有的参数都为一个非 0 的数,结果也是一样的。
\paragraph{}
通常初始参数为正负 ε 之间的随机值,假设我们要随机初始一个尺寸为 10×11 的参数矩阵,代码如下:
    \begin{minted}{octave}
     Theta1 = rand(10, 11) * (2*eps) - eps
    \end{minted}
\subsection{Putting it Together}
使用神经网络的步骤：
\paragraph{}
网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。
\paragraph{}
第一层的单元数即是我们训练集的特征数量。
\paragraph{}
最后一层的单元数是我们训练集的结果的类的数量。
\paragraph{}
如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。
\paragraph{}
我们真正要决定的是隐藏层的层数和每个中间层的单元数。
\subparagraph{}
训练神经网络：
\subparagraph{}
（1）参数的随机初始化
\subparagraph{}
（2）利用正向传播方法计算所有的$h_\Theta(x)$
\subparagraph{}
（3）编写计算代价函数$J(\Theta)$的代码
\subparagraph{}
（4）利用反向传播算法计算所有偏导数
\subparagraph{}
（5）利用数值检验方法检验这些偏导数
\subparagraph{}
（6）使用优化算法来最小化代价函数
\section{Application of Neural Networks}
\subsection{Autonomous Driving}
\end{CJK}
\end{document}