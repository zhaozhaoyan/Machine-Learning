\documentclass{article}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{geometry}
\geometry{a4paper,centering,scale=0.8}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
%可能用到的包
\title{Machine Learning - Week 7}
\author{赵燕}
\date{}
\begin{document} 
\hfuzz=\maxdimen
\tolerance=10000
\hbadness=10000
\begin{CJK}{UTF8}{gbsn} 
\maketitle
\renewcommand\contentsname{目录}
\renewcommand\figurename{图}
\tableofcontents
\newpage

\section{Large Margin Classification}
\subparagraph{}
人们有时将支持向量机看作是大间距分类器
\subsection{Optimization Objective}
\subparagraph{}
到目前为止,你已经见过一系列不同的学习算法。在监督学习中,许多学习算法的性能
都非常类似,因此,重要的不是你该选择使用学习算法 A 还是学习算法 B,而更重要的是,应用这些算法时,所创建的大量数据在应用这些算法时,表现情况通常依赖于你的水平。比如:你为学习算法所设计的特征量的选择,以及如何选择正则化参数,诸如此类的事。
\subparagraph{}
还有一个更加强大的算法广泛的应用于工业界和学术界,它被称为支持向量机(Support Vector Machine)。与逻辑回归和神经网络相比,支持向量机,或者简称SVM,在学习复杂的非线性方程时提供了一种更为清晰,更加强大的方式。因此,在接下来的视频中,我会探讨这一算法。在稍后的课程中,我也会对监督学习算法进行简要的总结。当然,仅仅是作简要描述。但对于支持向量机,鉴于该算法的强大和受欢迎度,在本课中,我会花许多时间来讲解它。它也是我们所介绍的最后一个监督学习算法。
\subparagraph{}
正如我们之前开发的学习算法,我们从优化目标开始。那么,我们开始学习这个算法。
\subparagraph{}
为了描述支持向量机,事实上,我将会从逻辑回归开始展示我们如何一点一点修改来得到本
质上的支持向量机。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{701.png}}
\caption{逻辑回归回顾}
\label{fig:701}
\end{figure}
\subparagraph{}
那么,在逻辑回归中我们已经熟悉了这里的假设函数形式,和右边的 S 型激励函数。然
而,为了解释一些数学知识.我将用 z 表示 $\theta^Tx$ 。
\subparagraph{}
现在考虑下我们想要逻辑回归做什么:如果有一个 y=1 的样本,我的意思是不管是在
训练集中或是在测试集中,又或者在交叉验证集中,总之是 y=1,现在我们希望 h(x) 趋近
1。因为我们想要正确地将此样本分类,这就意味着当 h(x) 趋近于 1 时, $\theta^Tx$ 应当远大于
0,这里的$\gg$意思是远远大于 0。这是因为由于 z 表示 $\theta^Tx$,当 z 远大于 0 时,即到了该图
的右边,你不难发现此时逻辑回归的输出将趋近于 1。相反地,如果我们有另一个样本,即
y=0。我们希望假设函数的输出值将趋近于 0,这对应于 $\theta^Tx$,或者就是 z 会远小于 0,因
为对应的假设函数的输出值趋近0。
\subparagraph{}
如果你进一步观察逻辑回归的代价函数,你会发现每个样本 (x, y)都会为总代价函数,
增加这里的一项,因此,对于总代价函数通常会有对所有的训练样本求和,并且这里还有一
个 $\frac{1}{m}$ 项,但是,在逻辑回归中,这里的这一项就是表示一个训练样本所对应的表达式。现
在,如果我将完整定义的假设函数代入这里。那么,我们就会得到每一个训练样本都影响这
一项。
\subparagraph{}
现在,先忽略$\frac{1}{m}$这一项,但是这一项是影响整个总代价函数中的这一项的。现在,一
起来考虑两种情况:一种是 y 等于 1 的情况;另一种是 y 等于 0 的情况。
\subparagraph{}
在第一种情况中,假设 y 等于 1,此时在目标函数中只需有第一项起作用,因为 y 等于 1 时,(1-y) 项将等于
0。因此,当在 y 等于 1 的样本中时,即在 (x, y) 中 y 等于 1,我们得到$-log(1-\frac{1}{{1+e^{-z}}})$。
\subparagraph{}
这样一项,这里同上一张图片一致。
\subparagraph{}
我用 z 表示$\theta^Tx$。当然,在代价函数中,y 前面有负号。我们只是这样表示,如果 y 等于 1 代价函数中,这一项也等于 1。这样做是为了简化此处的表达式。如果画出关于 z 的函数,你会看到左下角的这条曲线,我们同样可以看到,当 z 增大时,也就是相当于$\theta^Tx$ 增大时,z 对应的值会变的非常小。对整个代价函数而言,影响也非常小。这也就解释了,为什么逻辑回归在观察到正样本 y=1 时,试图将 $\theta^Tx$设置得非常大。因为,在代价函数中的这
一项会变的非常小。
\subparagraph{}
现在开始建立支持向量机,我们从这里开始:
\subparagraph{}
我们会从这个代价函数开始,也就是$-log(1-\frac{1}{{1+e^{-z}}})$一点一点修改,让我取这里的z=1 点,我先画出将要用的代价函数。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{703.png}}
\caption{支持向量机}
\label{fig:703}
\end{figure}
\subparagraph{}
新的代价函数将会水平的从这里到右边 (图外),然后我再画一条同逻辑回归非常相似
的直线,但是,在这里是一条直线,也就是我用紫红色画的曲线,就是这条紫红色的曲线。
那么,到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成,
即位于右边的水平部分和位于左边的直线部分,先别过多的考虑左边直线部分的斜率,这并不是很重要。但是,这里我们将使用的新的代价函数,是在 y=1 的前提下的。你也许能想到,这应该能做同逻辑回归中类似的事情,但事实上,在之后的的优化问题中,这会变得更坚定,并且为支持向量机,带来计算上的优势。例如,更容易计算股票交易的问题等等。
\subparagraph{}
目前,我们只是讨论了 y=1 的情况,另外一种情况是当 y=0 时,此时如果你仔细观察
代价函数只留下了第二项,因为第一项被消除了。如果当 y=0 时,那么这一项也就是 0 了。所以上述表达式只留下了第二项。因此,这个样本的代价或是代价函数的贡献。将会由这一项表示。并且,如果你将这一项作为 z 的函数,那么,这里就会得到横轴 z。现在,你完成了支持向量机中的部分内容,同样地,我们要替代这一条蓝色的线,用相似的方法。
\subparagraph{}
如果我们用一个新的代价函数来代替,即这条从 0 点开始的水平直线,然后是一条斜
线,像上图。那么,现在让我给这两个方程命名,左边的函数,称之为$cost_1(z)$,右边函数我称它为$cost_0(z)$,这里的下标是指在代价函数中,对应的 y=1 和 y=0 的情况,拥有了这些定义后,现在,我们就开始构建支持向量机。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{704.png}}
\caption{支持向量机代价函数}
\label{fig:704}
\end{figure}
\subparagraph{}
这是我们在逻辑回归中使用代价函数 J(θ)。也许这个方程看起来不是非常熟悉。这是因
为之前有个负号在方程外面,但是,这里我所做的是,将负号移到了表达式的里面,这样做
使得方程看起来有些不同。对于支持向量机而言,实质上我们要将这替换为$cost_1(z)$，也就是$cost_1(\theta^Tx)$,同样地,我也将这一项替换为$cost_0(z)$ ,也就是代价$cost_0(\theta^Tx)$。
这里的代价函数$cost_1$,就是之前所提到的那条线。此外,代价函数$cost_0$,也是上面所介绍过的那条线。因此,对于支持向量机,我们得到了这里的最小化问题,即:
\begin{equation}
min_{\theta}\frac{1}{m}[\sum_{i=1}^my^{(i)}(-logh_\theta(x^{(i)}))+(1-y^{(i)})(-log(1-h_\theta(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^n{\theta_j}^2
\end{equation}
\begin{equation}
min_{\theta}\frac{1}{m}[\sum_{i=1}^my^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{\lambda}{2m}\sum_{j=1}^n{\theta_j}^2
\end{equation}
\subparagraph{}
然后,再加上正则化参数。现在,按照支持向量机的惯例,事实上,我们的书写会稍微
有些不同,代价函数的参数表示也会稍微有些不同。
\subparagraph{}
首先,我们要除去 $\frac{1}{m}$ 这一项,当然,这仅仅是由于人们使用支持向量机时,对比于逻辑回归而言,不同的习惯所致,但这里我所说的意思是:你知道,我将要做的是仅仅除去$\frac{1}{m}$ 这一项,但是,这也会得出同样的 θ 最优值,好的,因为 $\frac{1}{m}$ 仅是个常量,因此,你知道在这个最小化问题中,无论前面是否有$\frac{1}{m}$ 这一项,最终我所得到的最优值 θ 都是一样的。这里我的意思是,先给你举一个实例,假定有一最小化问题:即要求当$ (u-5)^2+1 $取得
最小值时的 u 值,这时最小值为:当 u=5 时取得最小值。
\subparagraph{}
现在,如果我们想要将这个目标函数乘上常数 10,这里我的最小化问题就变成了:求
使得$ 10×(u-5)^2+10$ 最小的值 u,然而,使得这里最小的 u 值仍为 5。因此将一些常数乘以你的最小化项,这并不会改变最小化该方程时得到 u 值。因此,这里我所做的是删去常量m。也相同的,我将目标函数乘上一个常量 m,并不会改变取得最小值时的 θ 值。
\begin{equation}
min_{\theta}C[\sum_{i=1}^my^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n{\theta_j}^2
\end{equation}
\subparagraph{}
其中C=$\frac{1}{\lambda}$
\subparagraph{}
第二点概念上的变化,我们只是指在使用,支持向量机时,一些如下的标准惯例,而不是逻辑回归。因此,对于逻辑回归,在目标函数中,我们有两项:第一个是训练样本的代价,第二个是我们的正则化项,我们不得不去用这一项来平衡。这就相当于我们想要最小化 A 加
上正则化参数λ,然后乘以其他项 B 对吧?这里的 A 表示这里的第一项,同时我用 B 表示第二项,但不包括λ,我们不是优化这里的 A+λ×B。我们所做的是通过设置不同正则参数
λ 达到优化目的。这样,我们就能够权衡对应的项,是使得训练样本拟合的更好。即最小
化 A。还是保证正则参数足够小,也即是对于 B 项而言,但对于支持向量机,按照惯例,我
们将使用一个不同的参数替换这里使用的 λ来权衡这两项。你知道,就是第一项和第二项
我们依照惯例使用一个不同的参数称为 C,同时改为优化目标, C×A+B 因此,在逻辑回归中,如果给定λ,一个非常大的值,意味着给予 B 更大的权重。而这里,就对应于将 C 设定为非常小的值,那么,相应的将会给 B 比给 A 更大的权重。因此,这只是一种不同的方式来控制这种权衡或者一种不同的方法,即用参数来决定是更关心第一项的优化,还是更关心第二项的优化。当然你也可以把这里的参数 C 考虑成 $\frac{1}{\lambda}$,同 $\frac{1}{\lambda}$所扮演的角色相同,并且这两个方程或这两个表达式并不相同,因为 C 等于 $\frac{1}{\lambda}$,但是也并不全是这样,如果当 C 等于 $\frac{1}{\lambda}$时,这两个优化目标应当得到相同的值,相同的最优值 θ。因此,就用它们来代替。那么,我现在删掉这里的λ,并且用常数 C 来代替。因此,这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数,得到 SVM 学习到的参数 C。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{702.png}}
\label{fig:702}
\end{figure}
\subparagraph{}
最后有别于逻辑回归输出的概率。在这里,我们的代价函数,当最小化代价函数,获得
参数 θ 时,支持向量机所做的是它来直接预测 y 的值等于 1,还是等于 0。因此,这个假设函数会预测 1。当 $\theta^Tx$大于或者等于 0 时,或者等于 0 时,所以学习参数 θ 就是支持向量机假设函数的形式。那么,这就是支持向量机数学上的定义。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{705.png}}
\label{fig:705}
\end{figure}
\subsection{Large Margin Intuition}
\subparagraph{}
人们有时将支持向量机看作是大间距分类器。在这一部分,我将介绍其中的含义,这有助于我们直观理解 SVM 模型的假设是什么样的。
\subparagraph{}
这是我的支持向量机模型的代价函数,在左边这里我画出了关于 z 的代价函数$cost_1(z)$,此函数用于正样本,而在右边这里我画出了关于 z 的代价函数 $cost_0(z)$,横轴表示 z,现在让我们考虑一下,最小化这些代价函数的必要条件是什么。如果你有一个正样本,y 等于 1,
则只有在$z>=1$时,代价函数$cost_1(z)$)才等于 0。换句话说,如果你有一个正样本,我
们会希望 $\theta^Tx>=1$,反之,如果 y 是等于 0 的,我们观察一下,函数 cost 0 (z),它只有在$z<=1$的区间里函数值为 0。这是支持向量机的一个有趣性质。事实上,如果你有一个正样本 y 等于 1,则其实我们仅仅要求$\theta^Tx>=0$,就能将该样本恰当分出,这是因为如果 $\theta^Tx>0$的话,我们的模型代价函数值为 0,类似地,如果你有一个负样本,则仅需要 $\theta^Tx<=0$ 就会将负例正确分离,但是,支持向量机的要求更高,不仅仅要能正确分开输入的样本,即不仅仅要求$\theta^Tx>0$,我们需要的是比 0 值大很多,比如大于等于 1,我也想这个比 0 小很多,比如我希望它小于等于-1,这就相当于在支持向量机中嵌入了一个额外的安全因子。或者说安全的间距因子。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{707.png}}
\label{fig:707}
\caption{支持向量机算法}
\end{figure}
\subparagraph{}
当然,逻辑回归做了类似的事情。但是让我们看一下,在支持向量机中,这个因子会导致什么结果。具体而言,我接下来会考虑一个特例。我们将这个常数 C 设置成一个非常大的值。比如我们假设 C 的值为 100000 或者其它非常大的数,然后来观察支持向量机会给出什么结果?
\subparagraph{}
如果 C 非常大,则最小化代价函数的时候,我们将会很希望找到一个使第一项为 0 的最优解。因此,让我们尝试在代价项的第一项为 0 的情形下理解该优化问题。比如我们可以把 C 设置成了非常大的常数,这将给我们一些关于支持向量机模型的直观感受。
\begin{equation}
min_{\theta}C[\sum_{i=1}^my^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n{\theta_j}^2
\end{equation}
\subparagraph{}
我们已经看到输入一个训练样本标签为 y=1,你想令第一项为 0,你需要做的是找到一个 θ,使得$\theta^Tx>=1$,类似地,对于一个训练样本,标签为 y=0,为了使 cost 0 (z) 函数的值为0,我们需要$\theta^Tx<=-1$。因此,现在考虑我们的优化问题。选择参数,使得第一项等于 0,就会导致下面的优化问题,因为我们将选择参数使第一项为 0,因此这个函数的第一项为 0,因此是 C 乘以 0 加上二分之一乘以第二项。这里第一项是 C 乘以 0,因此可以将其删去,因为我知道它是 0。
\subparagraph{}
这将遵从以下的约束:$\theta^Tx^{(i)}>=1$,如果$y^{(i)}$是等于 1 的,$\theta^Tx^{(i)}<=-1$,如果样本 i 是一个负样本,这样当你求解这个优化问题的时候,当你最小化这个关于变量 θ 的函数的时候,你会得到一个非常有趣的决策边界。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{708.png}}
\label{fig:708}
\caption{支持向量机决策边界}
\end{figure}
\subparagraph{}
具体而言,如果你考察这样一个数据集,其中有正样本,也有负样本,可以看到这个数据集是线性可分的。我的意思是,存在一条直线把正负样本分开。当然有多条不同的直线,可以把正样本和负样本完全分开。
\subparagraph{}
比如,这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不是非常自然是么?
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{712.png}}
\label{fig:712}
\caption{支持向量机决策边界——线性分离情况}
\end{figure}
\subparagraph{}
或者我们可以画一条更差的决策界,这是另一条决策边界,可以将正样本和负样本分开,但仅仅是勉强分开,这些决策边界看起来都不是特别好的选择,支持向量机将会选择这个黑色的决策边界,相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多,黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲,这是什么意思呢?这条黑线有更大的距离,这个距离叫做间距 (margin)。
\subparagraph{}
当画出这两条额外的蓝线,我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近,在分离样本的时候就会比黑线表现差。因此,这个距
离叫做支持向量机的间距,而这是支持向量机具有鲁棒性的原因,因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器,而这其实是求解上一页幻灯片上优化问题的结果。
\subparagraph{}
我知道你也许想知道求解上一页幻灯片中的优化问题为什么会产生这个结果?它是如何产生这个大间距分类器的呢?我知道我还没有解释这一点。
\subparagraph{}
我将会从直观上略述为什么这个优化问题会产生大间距分类器。总之这个图示有助于你理解支持向量机模型的做法,即努力将正样本和负样本用最大的间距分开。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{709.png}}
\label{fig:709}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{710.png}}
\label{fig:710}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{711.png}}
\label{fig:711}
\end{figure}
\subparagraph{} 
在本节课中关于大间距分类器,我想讲最后一点:我们将这个大间距分类器中的正则化因子常数 C 设置的非常大,我记得我将其设置为了 100000,因此对这样的一个数据集,也许我们将选择这样的决策界,从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中,我们希望找出在 y=1 和 y=0 两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数,则我们的最小化问题便转变成:
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{716.png}}
\label{fig:716}
\end{figure}
\subparagraph{}
事实上,支持向量机现在要比这个大间距分类器所体现得更成熟,尤其是当你使用大间距分类器的时候,你的学习算法会受异常点 (outlier) 的影响。比如我们加入一个额外的正样本。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{715.png}}
\label{fig:715}
\caption{大间距分类器异常点的影响}
\end{figure}
\subparagraph{}
在这里,如果你加了这个样本,为了将样本用最大间距分开,也许我最终会得到一条类似这样的决策界,对么?就是这条粉色的线,仅仅基于一个异常值,仅仅基于一个样本,就将我的决策界从这条黑线变到这条粉线,这实在是不明智的。而如果正则化参数 C,设置的非常大,这事实上正是支持向量机将会做的。它将决策界,从黑线变到了粉线,但是如果 C设置的小一点,如果你将 C 设置的不要太大,则你最终会得到这条黑线,当然数据如果不是线性可分的,如果你在这里有一些正样本或者你在这里有一些负样本,则支持向量机也会将它们恰当分开。因此,大间距分类器的描述,仅仅是从直观上给出了正则化参数 C 非常大的情形,同时,要提醒你 C 的作用类似于 1/λ,λ是我们之前使用过的正则化参数。这只是C 非常大的情形,或者等价地λ非常小的情形。你最终会得到类似粉线这样的决策界,但是实际上应用支持向量机的时候,当 C 不是非常非常大的时候,它可以忽略掉一些异常点的影响,得到更好的决策界。甚至当你的数据不是线性可分的时候,支持向量机也可以给出好的结果。
\subparagraph{}
回顾 C=1/λ,因此:
\subparagraph{}
C 较大时,相当于 λ 较小,可能会导致过拟合,高方差。
\subparagraph{}
C 较小时,相当于 λ 较大,可能会导致欠拟合,高偏差。
\subparagraph{}
我们稍后会介绍支持向量机的偏差和方差,希望在那时候关于如何处理参数的这种平衡会变得更加清晰。我希望,这节课给出了一些关于为什么支持向量机被看做大间距分类器的直观理解。它用最大间距将样本区分开,尽管从技术上讲,这只有当参数 C 是非常大的时候是真的,但是它对于理解支持向量机是有益的。
本节课中我们略去了一步,那就是我们在幻灯片中给出的优化问题。为什么会是这样的?它是如何得出大间距分类器的?我在本节中没有讲解,在下一节课中,我将略述这些问题背后的数学原理,来解释这个优化问题是如何得到一个大间距分类器的。
\subsection{Mathematics Behind Large Margin Classification}
\subparagraph{}
在本节课中,我将介绍一些大间隔分类背后的数学原理。本节为选学部分,你完全可以跳过它,但是听听这节课可能让你对支持向量机中的优化问题,以及如何得到大间距分类器,产生更好的直观理解。
\subparagraph{}
首先,让我来给大家复习一下关于向量内积的知识。假设我有两个向量,u 和 v 我将它们写在这里。两个都是二维向量,我们看一下,$u^Tv$的结果。$u^Tv$也叫做向量 u 和 v 之间的
内积。由于是二维向量,我可以将它们画在这个图上。我们说,这就是向量 u 即在横轴上,取值为某个$u_1$,而在纵轴上,高度是某个$u_2$作为 u 的第二个分量。现在,很容易计算的一
个量就是向量 u 的范数。$\parallel{u}\parallel$表示 u 的范数,即 u 的长度,即向量 u 的欧几里得长度。根据
毕达哥拉斯定理,$\parallel{u}\parallel=\sqrt{{u_1}^2+{u_2}^2}$,这是向量 u 的长度,它是一个实数。现在你知道了这个的长度是多少了。我刚刚画的这个向量的长度就知道了。
\subparagraph{}
现在让我们回头来看向量 v ,因为我们想计算内积。v 是另一个向量,它的两个分量$v_1$和$v_2$是已知的。向量 v 可以画在这里,现在让我们来看看如何计算 u 和 v 之间的内积。
这就是具体做法,我们将向量 v 投影到向量 u 上,我们做一个直角投影,或者说一个 90 度投影将其投影到 u 上,接下来我度量这条红线的长度。我称这条红线的长度为 p,因此 p 就
是长度,或者说是向量 v 投影到向量 u 上的量,我将它写下来,p 是 v 投影到向量 u 上的长度,因此可以将 $u^Tv$=$p \cdot{u}$ ,或者说 u 的长度。这是计算内积的一种方法。如果你从几何上画出 p 的值,同时画出 u 的范数,你也会同样地计算出内积,答案是一样的。
\subparagraph{}
另一个计算公式是:$u^Tv$就是[u1 u2] 这个一行两列的矩阵乘以 v。因此可以得到 $u_1$×$v_1$+ $u_2$×$v_2$ 。根
据线性代数的知识,这两个公式会给出同样的结果。顺便说一句,$u^Tv$=$v^Tu$。因此如果你将u 和 v 交换位置,将 u 投影到 v 上,而不是将 v 投影到 u 上,然后做同样地计算,只是把 u和 v 的位置交换一下,你事实上可以得到同样的结果。声明一点,在这个等式中 u 的范数是一个实数,p 也是一个实数,因此 $u^Tv$就是两个实数正常相乘。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{717.png}}
\label{fig:717}
\caption{向量的内积}
\end{figure}
\subparagraph{}
最后一点,需要注意的就是 p 值,p 事实上是有符号的,即它可能是正值,也可能是负值。我的意思是说,如果 u 是一个类似这样的向量,v 是一个类似这样的向量,u 和 v 之间
的夹角大于 90 度,则如果将 v 投影到 u 上,会得到这样的一个投影,这是 p 的长度,在这个情形下我们仍然有$u^Tv$是等于 p 乘以 u 的范数。唯一一点不同的是 p 在这里是负的。在内积计算中,如果 u 和 v 之间的夹角小于 90 度,那么那条红线的长度 p 是正值。然而如果这个夹角大于 90 度,则 p 将会是负的。就是这个小线段的长度是负的。如果它们之间的夹
角大于 90 度,两个向量之间的内积也是负的。这就是关于向量内积的知识。我们接下来将会使用这些关于向量内积的性质试图来理解支持向量机中的目标函数。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{716.png}}
\label{fig:716}
\end{figure}
\subparagraph{}
这就是我们先前给出的支持向量机模型中的目标函数。为了讲解方便,我做一点简化,仅仅是为了让目标函数更容易被分析。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{718.png}}
\label{fig:718}
\end{figure}
\subparagraph{}
我接下来忽略掉截距,令 $\theta_0$ = 0,这样更容易画示意图。我将特征数 n 置为 2,因此我
们仅有两个特征 $x_1$和 $x_2$ ,现在 我们来看一下目标函数,支持向量机的优化目标函数。当我们仅有两个特征,
即 n=2 时,这个式子可以写作:
\begin{equation}
\frac{1}{2}({\theta_1}^2+{\theta_2}^2)=\frac{1}{2}(\sqrt{{\theta_1}^2+{\theta_2}^2})=\frac{1}{2}{\parallel{\theta}\parallel}^2
\end{equation}
我们只有两个参数 $\theta_1$和$\theta_2$。你可能注意到括号里面的这一项是向量 θ 的范数,或者说是向
量 θ 的长度。我的意思是如果我们将向量 θ 写出来,那么我刚刚画红线的这一项就是向量 θ的长度或范数。这里我们用的是之前学过的向量范数的定义,事实上这就等于向量 θ 的长度。
\subparagraph{}
当然你可以将其写作 $theta_0$ 、$\theta_1$、$\theta_2$,如果 $\theta_0$ 等于 0,那就是 $\theta_1\theta_2$ 的长度。在这里我将忽略
$theta_0$,这样来写 θ 的范数,它仅仅和  $\theta_1\theta_2$有关。但是,数学上不管你是否包含 θ 0 ,其实并没有
差别,因此在我们接下来的推导中去掉 $theta_0$ 不会有影响这意味着我们的目标函数是等于$\frac{1}{2}{\parallel{\theta}\parallel}^2$。因此支持向量机做的全部事情,就是极小化参数向量 θ 范数的平方,或者说长度
的平方。
\subparagraph{}
现在我将要看看这些项:$\theta^Tx$更深入地理解它们的含义。给定参数向量 θ 给定一个样本x,这等于什么呢? 在前一页幻灯片上,我们画出了在不同情形下,$u^Tx$的示意图,我们将
会使用这些概念,θ 和$x^{(i)}$就类似于 u 和 v 。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{720.png}}
\label{fig:720}
\end{figure}
\subparagraph{}
让我们看一下示意图:我们考察一个单一的训练样本,我有一个正样本在这里,用一个叉来表示这个样本$x^{(i)}$ ,意思是在水平轴上取值为$x_1^{(i)}$,在竖直轴上取值为$x_2^{(i)}$。这就是我画出的训练样本。尽管我没有将其真的看做向量。它事实上就是一个始于原点,终点位置在
这个训练样本点的向量。现在,我们有一个参数向量我会将它也画成向量。我将$\theta_1$ 画在横轴这里,将 $\theta_2$ 画在纵轴这里,那么内积$\theta^Tx^{(i)}$将会是什么呢?
\subparagraph{}
使用我们之前的方法,我们计算的方式就是我将训练样本投影到参数向量 θ,然后我来看一看这个线段的长度,我将它画成红色。我将它称为$p^{(i)}$用来表示这是第 i 个训练样本在参数向量 θ 上的投影。根据我们之前幻灯片的内容,我们知道的是$\theta^Tx^{(i)}$将会等于 p 乘以向
量 θ 的长度或范数。这就等于$\theta_1\cdot{x_1^{(i)}}+\theta_2\cdot{x_2^{(i)}}$。这两种方式是等价的,都可以用来计算θ 和$x^{(i)}$之间的内积。
\subparagraph{}
这告诉了我们什么呢?这里表达的意思是:这个$\theta^Tx^{(i)}\geq{1}$或者$\theta^Tx^{(i)}\leq{-1}$的，约束是可以被$p^{(i)}\cdot{x}\geq{1}$这个约束所代替的。因为$\theta^Tx^{(i)}=p^{(i)}\cdot{\parallel{\theta}\parallel}$，将其写入我们的优化目标。我们将会得到没有了约束，$\theta^Tx^{(i)}$而变成了$p^{(i)}\cdot{\parallel{\theta}\parallel}$。
\subparagraph{}
需要提醒一点,我们之前曾讲过这个优化目标函数可以被写成等于$\frac{1}{2}{\parallel{\theta}\parallel}^2$。
\begin{figure}[H]
\center{\includegraphics[width=.8\textwidth]{721.png}}
\label{fig:721}
\end{figure}
\subparagraph{}
现在让我们考虑下面这里的训练样本。现在,继续使用之前的简化,即$\theta_0$=0，我们来看一下支持向量机会选择什么样的决策界。这是一种选择,我们假设支持向量机会选择这个
决策边界。这不是一个非常好的选择,因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。
\subparagraph{}
对于这样选择的参数 θ,可以看到参数向量 θ 事实上是和决策界是 90 度正交的,因此这个绿色的决策界对应着一个参数向量 θ 指向这个方向,顺便提一句 $\theta_0$=0 的简化仅仅意味
着决策界必须通过原点 (0,0)。现在让我们看一下这对于优化目标函数意味着什么。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{722.png}}
\label{fig:722}
\end{figure}
比如这个样本,我们假设它是我的第一个样本 $x^{(1)}$,如果我考察这个样本到参数 θ 的投影,投影是这个短的红线段,就等于 $p^{(1)}$,它非常短。类似地,这个样本如果它恰好是 $x^{(2)}$ ,我的第二个训练样本,则它到 θ 的投影在这里。我将它画成粉色,这个短的粉色线段是$p^{(2)}$,即第二个样本到我的参数向量 θ 的投影。因此,这个投影非常短。$p^{(2)}$事实上是一个负值,$p^{(2)}$是在相反的方向,这个向量和参数向量 θ 的夹角大于 90 度,$p^{(2)}$的值小于 0。
\subparagraph{}
我们会发现这些$p^{(i)}$将会是非常小的数,因此当我们考察优化目标函数的时候,对于正样本而言,我们需要$p^{(i)}\cdot{\parallel{\theta}\parallel}\geq{1}$,但是如果$p^{(i)}$在这里非常小,那就意味着我们需要 θ 的范数
非常大.因为如果 $p^{(1)}$很小,而我们希望$p^{(1)}\cdot{\parallel{\theta}\parallel}\geq{1}$,令其实现的唯一的办法就是这两个数较大。如果 $p^{(1)}$小,我们就希望 θ 的范数大。类似地,对于负样本而言我们需要$p^{(2)}\cdot{\parallel{\theta}\parallel}\leq{-1}$,我们已经在这个样本中看到 $p^{(2)}$会是一个非常小的数,因此唯一的办法就是 θ 的范数变大。但是我们的目标函数是希望找到一个参数 θ,它的范数是小的。因此,这看起来不像是一个好的参数向量 θ 的选择。
\subparagraph{}
\begin{figure}[H]
\center{\includegraphics[width=.5\textwidth]{723.png}}
\label{fig:723}
\end{figure}
相反的,来看一个不同的决策边界。比如说,支持向量机选择了这个决策界,现在状况会有很大不同。如果这是决策界,这就是相对应的参数 θ 的方向,因此,在这个决策界之下,
垂直线是决策界。使用线性代数的知识,可以说明,这个绿色的决策界有一个垂直于它的向量 θ。现在如果你考察你的数据在横轴 x 上的投影,比如这个我之前提到的样本,我的样本
$x^{(1)}$,当我将它投影到横轴 x 上,或说投影到 θ 上,就会得到这样的 $p^{(1)}$。它的长度是 $p^{(1)}$,另一个样本,那个样本是 $x^{(2)}$。我做同样的投影,我会发现, $p^{(2)}$ 的长度是负值。你会注意到现在  $p^{(1)}$和  $p^{(2)}$这些投影长度是长多了。如果我们仍然要满足这些约束$p^{(1)}\cdot{\parallel{\theta}\parallel}\geq{1}$,则因为$p^{(1)}$变大了,θ 的范数就可以变小了。因此这意味着通过选择右边的决策界,而不是左边的那个,支持向量机可以使参数 θ 的范数变小很多。因此,如果我们想令 θ 的范数变小,从而令 θ 范数的平方变小,就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。
\subparagraph{}
看这条绿线,这个绿色的决策界。我们希望正样本和负样本投影到 θ 的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是 $p^{(1)}p^{(2)}p^{(3)}$等等的值。通过让间距变大,即通过这些 $p^{(1)}p^{(2)}p^{(3)}$ 等等的值,支持向量机最终可以找到一个较小的 θ 范数。这正是支持向量机中最小化目标函数的目的。
\subparagraph{}
以上就是为什么支持向量机最终会找到大间距分类器的原因。
因为它试图极大化这些$p^{(i)}$的范数,它们是训练样本到决策边界的距离。最后一点,我们的推导自始至终使用了这个简
化假设,就是参数$\theta_0$=0。
\subparagraph{}
就像我之前提到的。这个的作用是:$\theta_0$= 0 的意思是我们让决策界通过原点。如果你令$\theta_0$不是 0 的话,含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上,支持向量机产生大间距分类器的结论,会被证明同样成立,证明方式是非常类似的,是我们刚刚做的证明的推广。
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{724.png}}
\label{fig:724}
\end{figure}
\subparagraph{}
之前视频中说过,即便$\theta_0$不等于 0,支持向量机要做的事情都是优化这个目标函数对应着 C 值非常大的情况,但是可以说明的是,即便$\theta_0$不等于 0,支持向量机仍然会找到正样本和负样本之间的大间距分隔。
\subparagraph{}
总之,我们解释了为什么支持向量机是一个大间距分类器。在下一节我们,将开始讨论如何利用支持向量机的原理,应用它们建立一个复杂的非线性分类器。
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{725.png}}
\label{fig:725}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.4\textwidth]{726.png}}
\label{fig:726}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=.6\textwidth]{727.png}}
\label{fig:727}
\end{figure}
\section{Kernels}
\subsection{Kernels I}
\subsection{Kernels II}
\section{SVMs in Practice}
\subsection{Using An SVM}
\end{CJK}
\end{document}